{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtMxdgb9xG8n",
        "outputId": "217eb3f3-23b9-46e0-b4c6-1a7d18d51eb9"
      },
      "outputs": [],
      "source": [
        "# %pip install torch_geometric\n",
        "# %pip install torch_scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p4111OktwECj"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# 1) Imports\n",
        "########################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "import math\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k73rVZwOxU3R"
      },
      "outputs": [],
      "source": [
        "# GCN File from the git application of proivided paper at\n",
        "# https://github.com/seongjunyun/Graph_Transformer_Networks/blob/master/gcn.py + additional functions from other files\n",
        "\n",
        "from torch.nn import Parameter\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.nn.conv.message_passing import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops\n",
        "\n",
        "def glorot(tensor):\n",
        "    stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
        "    if tensor is not None:\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "def zeros(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
        "    Classfication with Graph Convolutional Networks\"\n",
        "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
        "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
        "\n",
        "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
        "    adjacency matrix with inserted self-loops and\n",
        "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
        "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
        "            (default: :obj:`False`)\n",
        "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
        "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
        "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}`.\n",
        "            (default: :obj:`False`)\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 improved=False,\n",
        "                 cached=False,\n",
        "                 bias=True,\n",
        "                 args=None):\n",
        "        super(GCNConv, self).__init__('add', flow='target_to_source')\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.improved = improved\n",
        "        self.cached = cached\n",
        "        self.cached_result = None\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.args = args\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.weight)\n",
        "        zeros(self.bias)\n",
        "        self.cached_result = None\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None, args=None):\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1), ),\n",
        "                                     dtype=dtype,\n",
        "                                     device=edge_index.device)\n",
        "        edge_weight = edge_weight.view(-1)\n",
        "        assert edge_weight.size(0) == edge_index.size(1)\n",
        "\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
        "\n",
        "        loop_weight = torch.full((num_nodes, ),\n",
        "                                1, # if not args.remove_self_loops else 0,\n",
        "                                dtype=edge_weight.dtype,\n",
        "                                device=edge_weight.device)\n",
        "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
        "\n",
        "        row, col = edge_index\n",
        "\n",
        "        # deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-1)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        # return edge_index, (deg_inv_sqrt[col] ** 0.5) * edge_weight * (deg_inv_sqrt[row] ** 0.5)\n",
        "        return edge_index, deg_inv_sqrt[row] * edge_weight\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        \"\"\"\"\"\"\n",
        "        x = torch.matmul(x, self.weight)\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
        "                                         self.improved, x.dtype, args=self.args)\n",
        "            self.cached_result = edge_index, norm\n",
        "        edge_index, norm = self.cached_result\n",
        "\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        if self.bias is not None:\n",
        "            aggr_out = aggr_out + self.bias\n",
        "        return aggr_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcd-M8-2zEsr",
        "outputId": "70fb94d8-f097-4a72-829d-be51e1ce75c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://www.dropbox.com/s/feo9qle74kg48gy/molecules.zip?dl=1\n",
            "Extracting ../data/ZINC/molecules.zip\n",
            "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/train.index\n",
            "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/val.index\n",
            "Downloading https://raw.githubusercontent.com/graphdeeplearning/benchmarking-gnns/master/data/molecules/test.index\n",
            "Processing...\n",
            "Processing train dataset: 100%|██████████| 10000/10000 [00:01<00:00, 9827.12it/s]\n",
            "Processing val dataset: 100%|██████████| 1000/1000 [00:00<00:00, 2960.04it/s]\n",
            "Processing test dataset: 100%|██████████| 1000/1000 [00:00<00:00, 6821.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 10000  Val size: 1000  Test size: 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "########################################\n",
        "# 2) Load the ZINC dataset from PyG\n",
        "########################################\n",
        "\n",
        "root = '../data/ZINC'\n",
        "train_dataset = ZINC(root, split='train', subset=True)\n",
        "val_dataset   = ZINC(root, split='val',   subset=True)\n",
        "test_dataset  = ZINC(root, split='test',  subset=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}  Val size: {len(val_dataset)}  Test size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jWEUGEXixXjX"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# 3) Define a simple GCN-based Model\n",
        "########################################\n",
        "#\n",
        "\n",
        "class GCNNet(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_layers, in_dim, out_dim=1):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        # First GCNConv: from in_dim -> hidden_dim\n",
        "        self.convs.append(GCNConv(in_channels=in_dim, out_channels=hidden_dim))\n",
        "\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(in_channels=hidden_dim, out_channels=hidden_dim))\n",
        "\n",
        "        self.lin = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
        "\n",
        "        for conv in self.convs:\n",
        "\n",
        "\n",
        "            out = conv(x, edge_index, edge_weight=None)\n",
        "            out = F.relu(out)\n",
        "            x = out\n",
        "\n",
        "        out_pool = global_mean_pool(x, batch)\n",
        "\n",
        "        y = self.lin(out_pool)  # shape [num_graphs, 1]\n",
        "        return y.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7QFeIENozgJ_"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# 3) Define a simple GCN-based Model\n",
        "########################################\n",
        "\n",
        "\n",
        "class GCNNet(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_layers, in_dim, out_dim=1):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        # First GCNConv: from in_dim -> hidden_dim\n",
        "        self.convs.append(GCNConv(in_channels=in_dim, out_channels=hidden_dim))\n",
        "\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(in_channels=hidden_dim, out_channels=hidden_dim))\n",
        "\n",
        "        self.lin = nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
        "\n",
        "\n",
        "        for conv in self.convs:\n",
        "\n",
        "            out = conv(x, edge_index, edge_weight=None)\n",
        "            out = F.relu(out)\n",
        "            x = out\n",
        "\n",
        "\n",
        "        out_pool = global_mean_pool(x, batch)\n",
        "\n",
        "        y = self.lin(out_pool)  # shape [num_graphs, 1]\n",
        "        return y.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9jm4Tj06zhJM"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "# 4) Training / Evaluation\n",
        "########################################\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_data in loader:\n",
        "        batch_data = batch_data.to(device)\n",
        "\n",
        "        y_pred = model(batch_data.x.float(), batch_data.edge_index, batch_data.batch, edge_attr=batch_data.edge_attr)\n",
        "        y_true = batch_data.y.view(-1).float()\n",
        "\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch_data in loader:\n",
        "        batch_data = batch_data.to(device)\n",
        "        y_pred = model(batch_data.x.float(), batch_data.edge_index, batch_data.batch, edge_attr=batch_data.edge_attr)\n",
        "        y_true = batch_data.y.view(-1).float()\n",
        "\n",
        "        loss = criterion(y_pred, y_true)\n",
        "        total_loss += loss.item() * batch_data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMeKg8uXzkAR",
        "outputId": "242ae4f1-b0ed-4185-9384-e3853948f733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 3.3809, Val Loss: 2.9242, Train MSE: 3.3809, Val MSE: 2.9242, Epochs no improvement: 0\n",
            "Epoch 2, Train Loss: 2.9346, Val Loss: 2.9171, Train MSE: 2.9346, Val MSE: 2.9171, Epochs no improvement: 0\n",
            "Epoch 3, Train Loss: 2.9212, Val Loss: 2.8817, Train MSE: 2.9212, Val MSE: 2.8817, Epochs no improvement: 0\n",
            "Epoch 4, Train Loss: 2.9070, Val Loss: 2.8668, Train MSE: 2.9070, Val MSE: 2.8668, Epochs no improvement: 0\n",
            "Epoch 5, Train Loss: 2.9000, Val Loss: 2.8677, Train MSE: 2.9000, Val MSE: 2.8677, Epochs no improvement: 0\n",
            "Epoch 6, Train Loss: 2.8967, Val Loss: 2.8626, Train MSE: 2.8967, Val MSE: 2.8626, Epochs no improvement: 1\n",
            "Epoch 7, Train Loss: 2.8747, Val Loss: 2.8330, Train MSE: 2.8747, Val MSE: 2.8330, Epochs no improvement: 0\n",
            "Epoch 8, Train Loss: 2.8557, Val Loss: 2.7785, Train MSE: 2.8557, Val MSE: 2.7785, Epochs no improvement: 0\n",
            "Epoch 9, Train Loss: 2.7995, Val Loss: 2.8264, Train MSE: 2.7995, Val MSE: 2.8264, Epochs no improvement: 0\n",
            "Epoch 10, Train Loss: 2.7572, Val Loss: 2.6890, Train MSE: 2.7572, Val MSE: 2.6890, Epochs no improvement: 1\n",
            "Epoch 11, Train Loss: 2.7138, Val Loss: 2.6700, Train MSE: 2.7138, Val MSE: 2.6700, Epochs no improvement: 0\n",
            "Epoch 12, Train Loss: 2.6844, Val Loss: 2.6616, Train MSE: 2.6844, Val MSE: 2.6616, Epochs no improvement: 0\n",
            "Epoch 13, Train Loss: 2.6654, Val Loss: 2.6133, Train MSE: 2.6654, Val MSE: 2.6133, Epochs no improvement: 0\n",
            "Epoch 14, Train Loss: 2.6672, Val Loss: 2.5948, Train MSE: 2.6672, Val MSE: 2.5948, Epochs no improvement: 0\n",
            "Epoch 15, Train Loss: 2.6407, Val Loss: 2.6417, Train MSE: 2.6407, Val MSE: 2.6417, Epochs no improvement: 0\n",
            "Epoch 16, Train Loss: 2.6323, Val Loss: 2.5965, Train MSE: 2.6323, Val MSE: 2.5965, Epochs no improvement: 1\n",
            "Epoch 17, Train Loss: 2.6225, Val Loss: 2.6014, Train MSE: 2.6225, Val MSE: 2.6014, Epochs no improvement: 2\n",
            "Epoch 18, Train Loss: 2.6297, Val Loss: 2.5697, Train MSE: 2.6297, Val MSE: 2.5697, Epochs no improvement: 3\n",
            "Epoch 19, Train Loss: 2.6126, Val Loss: 2.5688, Train MSE: 2.6126, Val MSE: 2.5688, Epochs no improvement: 0\n",
            "Epoch 20, Train Loss: 2.6105, Val Loss: 2.6188, Train MSE: 2.6105, Val MSE: 2.6188, Epochs no improvement: 0\n",
            "Epoch 21, Train Loss: 2.6041, Val Loss: 2.5834, Train MSE: 2.6041, Val MSE: 2.5834, Epochs no improvement: 1\n",
            "Epoch 22, Train Loss: 2.6020, Val Loss: 2.5577, Train MSE: 2.6020, Val MSE: 2.5577, Epochs no improvement: 2\n",
            "Epoch 23, Train Loss: 2.5996, Val Loss: 2.5522, Train MSE: 2.5996, Val MSE: 2.5522, Epochs no improvement: 0\n",
            "Epoch 24, Train Loss: 2.5811, Val Loss: 2.5463, Train MSE: 2.5811, Val MSE: 2.5463, Epochs no improvement: 0\n",
            "Epoch 25, Train Loss: 2.5652, Val Loss: 2.5978, Train MSE: 2.5652, Val MSE: 2.5978, Epochs no improvement: 0\n",
            "Epoch 26, Train Loss: 2.5835, Val Loss: 2.5278, Train MSE: 2.5835, Val MSE: 2.5278, Epochs no improvement: 1\n",
            "Epoch 27, Train Loss: 2.5455, Val Loss: 2.5055, Train MSE: 2.5455, Val MSE: 2.5055, Epochs no improvement: 0\n",
            "Epoch 28, Train Loss: 2.5399, Val Loss: 2.4978, Train MSE: 2.5399, Val MSE: 2.4978, Epochs no improvement: 0\n",
            "Epoch 29, Train Loss: 2.5300, Val Loss: 2.4752, Train MSE: 2.5300, Val MSE: 2.4752, Epochs no improvement: 0\n",
            "Epoch 30, Train Loss: 2.5126, Val Loss: 2.4695, Train MSE: 2.5126, Val MSE: 2.4695, Epochs no improvement: 0\n",
            "Epoch 31, Train Loss: 2.5078, Val Loss: 2.4589, Train MSE: 2.5078, Val MSE: 2.4589, Epochs no improvement: 0\n",
            "Epoch 32, Train Loss: 2.4872, Val Loss: 2.4427, Train MSE: 2.4872, Val MSE: 2.4427, Epochs no improvement: 0\n",
            "Epoch 33, Train Loss: 2.4622, Val Loss: 2.4101, Train MSE: 2.4622, Val MSE: 2.4101, Epochs no improvement: 0\n",
            "Epoch 34, Train Loss: 2.4487, Val Loss: 2.4440, Train MSE: 2.4487, Val MSE: 2.4440, Epochs no improvement: 0\n",
            "Epoch 35, Train Loss: 2.4383, Val Loss: 2.3873, Train MSE: 2.4383, Val MSE: 2.3873, Epochs no improvement: 1\n",
            "Epoch 36, Train Loss: 2.4246, Val Loss: 2.4777, Train MSE: 2.4246, Val MSE: 2.4777, Epochs no improvement: 0\n",
            "Epoch 37, Train Loss: 2.4076, Val Loss: 2.4634, Train MSE: 2.4076, Val MSE: 2.4634, Epochs no improvement: 1\n",
            "Epoch 38, Train Loss: 2.3949, Val Loss: 2.3889, Train MSE: 2.3949, Val MSE: 2.3889, Epochs no improvement: 2\n",
            "Epoch 39, Train Loss: 2.3810, Val Loss: 2.3375, Train MSE: 2.3810, Val MSE: 2.3375, Epochs no improvement: 3\n",
            "Epoch 40, Train Loss: 2.3936, Val Loss: 2.3304, Train MSE: 2.3936, Val MSE: 2.3304, Epochs no improvement: 0\n",
            "Epoch 41, Train Loss: 2.3634, Val Loss: 2.3789, Train MSE: 2.3634, Val MSE: 2.3789, Epochs no improvement: 0\n",
            "Epoch 42, Train Loss: 2.3720, Val Loss: 2.3365, Train MSE: 2.3720, Val MSE: 2.3365, Epochs no improvement: 1\n",
            "Epoch 43, Train Loss: 2.3488, Val Loss: 2.3738, Train MSE: 2.3488, Val MSE: 2.3738, Epochs no improvement: 2\n",
            "Epoch 44, Train Loss: 2.3612, Val Loss: 2.2956, Train MSE: 2.3612, Val MSE: 2.2956, Epochs no improvement: 3\n",
            "Epoch 45, Train Loss: 2.3304, Val Loss: 2.3312, Train MSE: 2.3304, Val MSE: 2.3312, Epochs no improvement: 0\n",
            "Epoch 46, Train Loss: 2.3158, Val Loss: 2.2928, Train MSE: 2.3158, Val MSE: 2.2928, Epochs no improvement: 1\n",
            "Epoch 47, Train Loss: 2.3414, Val Loss: 2.4029, Train MSE: 2.3414, Val MSE: 2.4029, Epochs no improvement: 0\n",
            "Epoch 48, Train Loss: 2.3121, Val Loss: 2.2640, Train MSE: 2.3121, Val MSE: 2.2640, Epochs no improvement: 1\n",
            "Epoch 49, Train Loss: 2.3001, Val Loss: 2.3072, Train MSE: 2.3001, Val MSE: 2.3072, Epochs no improvement: 0\n",
            "Epoch 50, Train Loss: 2.3045, Val Loss: 2.2489, Train MSE: 2.3045, Val MSE: 2.2489, Epochs no improvement: 1\n",
            "Epoch 51, Train Loss: 2.2815, Val Loss: 2.2779, Train MSE: 2.2815, Val MSE: 2.2779, Epochs no improvement: 0\n",
            "Epoch 52, Train Loss: 2.2755, Val Loss: 2.2469, Train MSE: 2.2755, Val MSE: 2.2469, Epochs no improvement: 1\n",
            "Epoch 53, Train Loss: 2.2687, Val Loss: 2.2513, Train MSE: 2.2687, Val MSE: 2.2513, Epochs no improvement: 0\n",
            "Epoch 54, Train Loss: 2.2649, Val Loss: 2.2592, Train MSE: 2.2649, Val MSE: 2.2592, Epochs no improvement: 1\n",
            "Epoch 55, Train Loss: 2.3034, Val Loss: 2.3895, Train MSE: 2.3034, Val MSE: 2.3895, Epochs no improvement: 2\n",
            "Epoch 56, Train Loss: 2.2546, Val Loss: 2.2554, Train MSE: 2.2546, Val MSE: 2.2554, Epochs no improvement: 3\n",
            "Epoch 57, Train Loss: 2.2507, Val Loss: 2.2222, Train MSE: 2.2507, Val MSE: 2.2222, Epochs no improvement: 4\n",
            "Epoch 58, Train Loss: 2.2348, Val Loss: 2.2299, Train MSE: 2.2348, Val MSE: 2.2299, Epochs no improvement: 0\n",
            "Epoch 59, Train Loss: 2.2424, Val Loss: 2.2516, Train MSE: 2.2424, Val MSE: 2.2516, Epochs no improvement: 1\n",
            "Epoch 60, Train Loss: 2.2263, Val Loss: 2.3082, Train MSE: 2.2263, Val MSE: 2.3082, Epochs no improvement: 2\n",
            "Epoch 61, Train Loss: 2.2263, Val Loss: 2.2050, Train MSE: 2.2263, Val MSE: 2.2050, Epochs no improvement: 3\n",
            "Epoch 62, Train Loss: 2.2265, Val Loss: 2.1999, Train MSE: 2.2265, Val MSE: 2.1999, Epochs no improvement: 0\n",
            "Epoch 63, Train Loss: 2.2102, Val Loss: 2.2246, Train MSE: 2.2102, Val MSE: 2.2246, Epochs no improvement: 0\n",
            "Epoch 64, Train Loss: 2.2070, Val Loss: 2.2469, Train MSE: 2.2070, Val MSE: 2.2469, Epochs no improvement: 1\n",
            "Epoch 65, Train Loss: 2.2107, Val Loss: 2.2224, Train MSE: 2.2107, Val MSE: 2.2224, Epochs no improvement: 2\n",
            "Epoch 66, Train Loss: 2.2025, Val Loss: 2.2464, Train MSE: 2.2025, Val MSE: 2.2464, Epochs no improvement: 3\n",
            "Epoch 67, Train Loss: 2.1926, Val Loss: 2.1883, Train MSE: 2.1926, Val MSE: 2.1883, Epochs no improvement: 4\n",
            "Epoch 68, Train Loss: 2.2020, Val Loss: 2.2646, Train MSE: 2.2020, Val MSE: 2.2646, Epochs no improvement: 0\n",
            "Epoch 69, Train Loss: 2.2075, Val Loss: 2.1955, Train MSE: 2.2075, Val MSE: 2.1955, Epochs no improvement: 1\n",
            "Epoch 70, Train Loss: 2.1882, Val Loss: 2.3005, Train MSE: 2.1882, Val MSE: 2.3005, Epochs no improvement: 2\n",
            "Epoch 71, Train Loss: 2.2024, Val Loss: 2.2090, Train MSE: 2.2024, Val MSE: 2.2090, Epochs no improvement: 3\n",
            "Epoch 72, Train Loss: 2.1841, Val Loss: 2.2060, Train MSE: 2.1841, Val MSE: 2.2060, Epochs no improvement: 4\n",
            "Epoch 73, Train Loss: 2.1768, Val Loss: 2.2311, Train MSE: 2.1768, Val MSE: 2.2311, Epochs no improvement: 5\n",
            "Epoch 74, Train Loss: 2.1789, Val Loss: 2.2039, Train MSE: 2.1789, Val MSE: 2.2039, Epochs no improvement: 6\n",
            "Epoch 75, Train Loss: 2.1828, Val Loss: 2.1826, Train MSE: 2.1828, Val MSE: 2.1826, Epochs no improvement: 7\n",
            "Epoch 76, Train Loss: 2.1770, Val Loss: 2.2161, Train MSE: 2.1770, Val MSE: 2.2161, Epochs no improvement: 0\n",
            "Epoch 77, Train Loss: 2.1683, Val Loss: 2.2238, Train MSE: 2.1683, Val MSE: 2.2238, Epochs no improvement: 1\n",
            "Epoch 78, Train Loss: 2.1677, Val Loss: 2.1952, Train MSE: 2.1677, Val MSE: 2.1952, Epochs no improvement: 2\n",
            "Epoch 79, Train Loss: 2.1658, Val Loss: 2.1763, Train MSE: 2.1658, Val MSE: 2.1763, Epochs no improvement: 3\n",
            "Epoch 80, Train Loss: 2.1723, Val Loss: 2.2037, Train MSE: 2.1723, Val MSE: 2.2037, Epochs no improvement: 0\n",
            "Epoch 81, Train Loss: 2.1710, Val Loss: 2.1855, Train MSE: 2.1710, Val MSE: 2.1855, Epochs no improvement: 1\n",
            "Epoch 82, Train Loss: 2.1633, Val Loss: 2.1865, Train MSE: 2.1633, Val MSE: 2.1865, Epochs no improvement: 2\n",
            "Epoch 83, Train Loss: 2.1602, Val Loss: 2.1787, Train MSE: 2.1602, Val MSE: 2.1787, Epochs no improvement: 3\n",
            "Epoch 84, Train Loss: 2.1482, Val Loss: 2.1855, Train MSE: 2.1482, Val MSE: 2.1855, Epochs no improvement: 4\n",
            "Epoch 85, Train Loss: 2.1452, Val Loss: 2.3292, Train MSE: 2.1452, Val MSE: 2.3292, Epochs no improvement: 5\n",
            "Epoch 86, Train Loss: 2.1576, Val Loss: 2.1745, Train MSE: 2.1576, Val MSE: 2.1745, Epochs no improvement: 6\n",
            "Epoch 87, Train Loss: 2.1414, Val Loss: 2.2378, Train MSE: 2.1414, Val MSE: 2.2378, Epochs no improvement: 0\n",
            "Epoch 88, Train Loss: 2.1395, Val Loss: 2.1827, Train MSE: 2.1395, Val MSE: 2.1827, Epochs no improvement: 1\n",
            "Epoch 89, Train Loss: 2.1466, Val Loss: 2.1878, Train MSE: 2.1466, Val MSE: 2.1878, Epochs no improvement: 2\n",
            "Epoch 90, Train Loss: 2.1329, Val Loss: 2.1582, Train MSE: 2.1329, Val MSE: 2.1582, Epochs no improvement: 3\n",
            "Epoch 91, Train Loss: 2.1502, Val Loss: 2.1539, Train MSE: 2.1502, Val MSE: 2.1539, Epochs no improvement: 0\n",
            "Epoch 92, Train Loss: 2.1214, Val Loss: 2.1685, Train MSE: 2.1214, Val MSE: 2.1685, Epochs no improvement: 0\n",
            "Epoch 93, Train Loss: 2.1432, Val Loss: 2.1818, Train MSE: 2.1432, Val MSE: 2.1818, Epochs no improvement: 1\n"
          ]
        }
      ],
      "source": [
        "########################################\n",
        "# 5) Putting It All Together\n",
        "########################################\n",
        "\n",
        "model = GCNNet(\n",
        "    hidden_dim=256,\n",
        "    num_layers=5,  # e.g. 3-layer GCN\n",
        "    in_dim=train_dataset.num_node_features,  # Usually 28\n",
        "    out_dim=1\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "patience = 100\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "best_model_state = None\n",
        "\n",
        "epochs = 2000\n",
        "for epoch in range(1, epochs+1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss   = evaluate(model, val_loader, criterion)\n",
        "    # if epoch % 2 == 0:\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MSE: {train_loss:.4f}, Val MSE: {val_loss:.4f}, Epochs no improvement: {epochs_no_improve}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "test_mse = evaluate(model, test_loader, criterion)\n",
        "print(f\"Final Test MSE: {test_mse:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GNN",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
