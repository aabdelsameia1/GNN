{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1) Imports\n",
    "########################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import math\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN File from https://github.com/seongjunyun/Graph_Transformer_Networks/blob/master/gcn.py + additional functions from other files\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.nn.conv.message_passing import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def glorot(tensor):\n",
    "    stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "def zeros(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0)\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classfication with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`{\\left(\\mathbf{\\hat{D}}^{-1/2}\n",
    "            \\mathbf{\\hat{A}} \\mathbf{\\hat{D}}^{-1/2} \\right)}`.\n",
    "            (default: :obj:`False`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 improved=False,\n",
    "                 cached=False,\n",
    "                 bias=True,\n",
    "                 args=None):\n",
    "        super(GCNConv, self).__init__('add', flow='target_to_source')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.args = args\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        self.cached_result = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, improved=False, dtype=None, args=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1), ),\n",
    "                                     dtype=dtype,\n",
    "                                     device=edge_index.device)\n",
    "        edge_weight = edge_weight.view(-1)\n",
    "        assert edge_weight.size(0) == edge_index.size(1)\n",
    "\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "        \n",
    "        loop_weight = torch.full((num_nodes, ),\n",
    "                                1, # if not args.remove_self_loops else 0,\n",
    "                                dtype=edge_weight.dtype,\n",
    "                                device=edge_weight.device)\n",
    "        edge_weight = torch.cat([edge_weight, loop_weight], dim=0)\n",
    "\n",
    "        row, col = edge_index\n",
    "        \n",
    "        # deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-1)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        # return edge_index, (deg_inv_sqrt[col] ** 0.5) * edge_weight * (deg_inv_sqrt[row] ** 0.5)\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\"\"\"\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n",
    "                                         self.improved, x.dtype, args=self.args)\n",
    "            self.cached_result = edge_index, norm\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10000  Val size: 1000  Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 2) Load the ZINC dataset from PyG\n",
    "########################################\n",
    "# By default, subset=True uses ~12k molecules, each is a separate graph.\n",
    "# The data object has .x (node features), .edge_index, .edge_attr, .y\n",
    "\n",
    "root = '../data/ZINC'\n",
    "train_dataset = ZINC(root, split='train', subset=True)\n",
    "val_dataset   = ZINC(root, split='val',   subset=True)\n",
    "test_dataset  = ZINC(root, split='test',  subset=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}  Val size: {len(val_dataset)}  Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3) Define a simple GCN-based Model\n",
    "########################################\n",
    "# We'll adapt a standard \"Graph-level\" GCN model:\n",
    "#   2 or 3 layers of GCNConv from gcn.py\n",
    "#   global mean pool\n",
    "#   final MLP for the property (ZINC is regression, so out_dim=1)\n",
    "########################################\n",
    "\n",
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, in_dim, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # First GCNConv: from in_dim -> hidden_dim\n",
    "        self.convs.append(GCNConv(in_channels=in_dim, out_channels=hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(in_channels=hidden_dim, out_channels=hidden_dim))\n",
    "        \n",
    "        self.lin = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # x: [N, in_dim]\n",
    "        # edge_index: [2, E]\n",
    "        # batch: [N] (which graph each node belongs to)\n",
    "        # edge_attr: can pass as edge_weight if 1D\n",
    "        # or we can do modifications inside GCNConv if needed.\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            # The original GCNConv from your code expects edge_weight:\n",
    "            # So if ZINC's edge_attr is not scalar, you might do some aggregation or pass None.\n",
    "            \n",
    "            out = conv(x, edge_index, edge_weight=None)\n",
    "            out = F.relu(out)\n",
    "            x = out\n",
    "        \n",
    "        # Then pool to get graph-level embedding\n",
    "        out_pool = global_mean_pool(x, batch)  # shape [num_graphs, hidden_dim]\n",
    "        \n",
    "        # Final linear for regression\n",
    "        y = self.lin(out_pool)  # shape [num_graphs, 1]\n",
    "        return y.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 4) Training / Evaluation\n",
    "########################################\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        # batch_data.x: [N, node_features]\n",
    "        # batch_data.edge_index: [2, E]\n",
    "        # batch_data.edge_attr: [E, bond_dim] (ZINC)\n",
    "        # batch_data.y: [batch_size_of_graphs, 1]\n",
    "        # batch_data.batch: [N] => which graph each node belongs to\n",
    "        \n",
    "        y_pred = model(batch_data.x.float(), batch_data.edge_index, batch_data.batch, edge_attr=batch_data.edge_attr)\n",
    "        y_true = batch_data.y.view(-1).float()\n",
    "        \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        y_pred = model(batch_data.x.float(), batch_data.edge_index, batch_data.batch, edge_attr=batch_data.edge_attr)\n",
    "        y_true = batch_data.y.view(-1).float()\n",
    "        \n",
    "        loss = criterion(y_pred, y_true)\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.1930, Val Loss: 2.9130, Train MSE: 3.1930, Val MSE: 2.9130, Epochs no improvement: 0\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 5) Putting It All Together\n",
    "########################################\n",
    "\n",
    "model = GCNNet(\n",
    "    hidden_dim=64, \n",
    "    num_layers=3,  # e.g. 3-layer GCN\n",
    "    in_dim=train_dataset.num_node_features,  # Usually 28\n",
    "    out_dim=1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss   = evaluate(model, val_loader, criterion)\n",
    "    # if epoch % 2 == 0:\n",
    "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MSE: {train_loss:.4f}, Val MSE: {val_loss:.4f}, Epochs no improvement: {epochs_no_improve}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "test_mse = evaluate(model, test_loader, criterion)\n",
    "print(f\"Final Test MSE: {test_mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
