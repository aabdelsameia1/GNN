{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Inferred in_channels: 1\n"
     ]
    }
   ],
   "source": [
    "# src/data_utils.py\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_zinc_dataset(root='../data/ZINC', batch_size=64, subset=True):\n",
    "    \"\"\"\n",
    "    Loads the ZINC dataset from the specified root directory.\n",
    "    \n",
    "    Args:\n",
    "        root (str): Path to the dataset folder.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "    \n",
    "    Returns:\n",
    "        (DataLoader, DataLoader, DataLoader): train, val, and test loaders.\n",
    "    \"\"\"\n",
    "    train_dataset = ZINC(root, split='train', subset=subset)\n",
    "    val_dataset = ZINC(root, split='val', subset=subset)\n",
    "    test_dataset = ZINC(root, split='test', subset=subset)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "def get_activation_fn(name):\n",
    "    name = name.lower()\n",
    "    if name == 'relu':\n",
    "        return F.relu\n",
    "    elif name == 'leakyrelu':\n",
    "        return F.leaky_relu\n",
    "    elif name == 'elu':\n",
    "        return F.elu\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {name}\")\n",
    "\n",
    "\n",
    "def get_activation_module(name):\n",
    "    \"\"\"\n",
    "    For usage in nn.Sequential, we need an nn.Module (e.g. nn.ReLU).\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    if name == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif name == 'leakyrelu':\n",
    "        return nn.LeakyReLU(negative_slope=0.2)\n",
    "    elif name == 'elu':\n",
    "        return nn.ELU()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation module: {name}\")\n",
    "\n",
    "\n",
    "# src/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, \n",
    "    GINConv, \n",
    "    GATConv, \n",
    "    SAGEConv,\n",
    "    GINEConv,\n",
    "    global_mean_pool,\n",
    "    global_max_pool,\n",
    "    GlobalAttention\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# Pooling Helpers\n",
    "###############################################################################\n",
    "def get_pooling_fn(pool_type, hidden_dim):\n",
    "    pool_type = pool_type.lower()\n",
    "    if pool_type == 'mean':\n",
    "        return global_mean_pool\n",
    "    elif pool_type == 'max':\n",
    "        return global_max_pool\n",
    "    elif pool_type == 'attention':\n",
    "        gate_nn = nn.Sequential(nn.Linear(hidden_dim, 1))\n",
    "        return GlobalAttention(gate_nn)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling type: {pool_type}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1. GCN Model (no edge features)\n",
    "###############################################################################\n",
    "class GCNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible 2-layer GCN supporting dropout, batch norm, residual, etc.\n",
    "    This model ignores edge_attr entirely.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        hidden_dim=64, \n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # We ignore edge_attr for standard GCN\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "        \n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. GIN Model (no edge features)\n",
    "###############################################################################\n",
    "class GINModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard 2-layer GIN ignoring edge_attr.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(GINModel, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv1 = GINConv(self.mlp1)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv2 = GINConv(self.mlp2)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # We ignore edge_attr for standard GIN\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. GINE Model (USES edge features)\n",
    "###############################################################################\n",
    "class GINEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GINE variant that can incorporate edge_attr (e.g., bond types).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False,\n",
    "        edge_dim=None  # dimension of edge_attr, if known\n",
    "    ):\n",
    "        super(GINEModel, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # MLP for first GINEConv\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv1 = GINEConv(nn=self.mlp1, edge_dim=edge_dim if edge_dim else 0)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # MLP for second GINEConv\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv2 = GINEConv(nn=self.mlp2, edge_dim=edge_dim if edge_dim else 0)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # Fix dimension if edge_attr is 1D\n",
    "        if edge_attr is not None and edge_attr.dim() == 1:\n",
    "            edge_attr = edge_attr.unsqueeze(-1)\n",
    "        \n",
    "        # 1st GINEConv\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        # 2nd GINEConv\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. GAT Model (no edge features)\n",
    "###############################################################################\n",
    "class GATModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible 2-layer GAT supporting dropout, batch norm, residual, etc.\n",
    "    Ignores edge_attr unless you implement a custom attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        hidden_dim=64, \n",
    "        out_channels=1, \n",
    "        heads=4,\n",
    "        dropout=0.0, \n",
    "        activation='relu', \n",
    "        pool='mean', \n",
    "        residual=False, \n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(GATModel, self).__init__()\n",
    "        \n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.conv1 = GATConv(in_channels, hidden_dim, heads=heads, concat=True)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=True)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim * heads)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. GraphSAGE Model (no edge features)\n",
    "###############################################################################\n",
    "class SAGEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible 2-layer GraphSAGE supporting dropout, batch norm, residual, etc.\n",
    "    Ignores edge_attr unless you implement a custom aggregator.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(SAGEModel, self).__init__()\n",
    "        \n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6. Graph Transformer (no edge features by default)\n",
    "###############################################################################\n",
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Placeholder for a Graph Transformer approach.\n",
    "    Currently does not incorporate edge_attr in attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_dim=64, out_channels=1, num_heads=4):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.lin_in = nn.Linear(in_channels, hidden_dim)\n",
    "        self.lin_out = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # We ignore edge_index and edge_attr in this placeholder\n",
    "        x = self.lin_in(x)\n",
    "        x = x.unsqueeze(1)           # [num_nodes, 1, hidden_dim]\n",
    "        x = x.permute(0, 1, 2)       # [num_nodes, 1, hidden_dim]\n",
    "        x = self.encoder(x)          # [num_nodes, 1, hidden_dim]\n",
    "        x = x.squeeze(1)             # [num_nodes, hidden_dim]\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. Factory Method\n",
    "###############################################################################\n",
    "def get_model(\n",
    "    model_name, \n",
    "    in_channels, \n",
    "    hidden_dim=64, \n",
    "    out_channels=1,\n",
    "    dropout=0.0,\n",
    "    activation='relu',\n",
    "    pool='mean',\n",
    "    residual=False,\n",
    "    batch_norm=False,\n",
    "    heads=4,    # used for GAT/Transformer\n",
    "    edge_dim=None  # used for GINE\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns an instance of the requested model by name.\n",
    "    - 'gcn', 'gin', 'gat', 'sage', 'transformer' ignore edge_attr\n",
    "    - 'gine' uses edge_attr\n",
    "    \"\"\"\n",
    "    model_name = model_name.lower()\n",
    "\n",
    "    if model_name == 'gcn':\n",
    "        return GCNModel(\n",
    "            in_channels, \n",
    "            hidden_dim, \n",
    "            out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'gin':\n",
    "        return GINModel(\n",
    "            in_channels,\n",
    "            hidden_dim,\n",
    "            out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'gine':\n",
    "        return GINEModel(\n",
    "            in_channels,\n",
    "            hidden_dim,\n",
    "            out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm,\n",
    "            edge_dim=edge_dim\n",
    "        )\n",
    "    elif model_name == 'gat':\n",
    "        return GATModel(\n",
    "            in_channels=in_channels,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_channels=out_channels,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'sage':\n",
    "        return SAGEModel(\n",
    "            in_channels=in_channels,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_channels=out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'transformer':\n",
    "        return GraphTransformer(\n",
    "            in_channels,\n",
    "            hidden_dim,\n",
    "            out_channels,\n",
    "            num_heads=heads\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}.\")\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# If needed:\n",
    "# sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1. LOAD DATA ONCE FOR ALL MODELS\n",
    "###############################################################################\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = get_zinc_dataset(\n",
    "    root='../data/ZINC',\n",
    "    batch_size=batch_size,\n",
    "    subset=True\n",
    ")\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "in_channels = sample_batch.x.size(-1)\n",
    "print(f\"Inferred in_channels: {in_channels}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. UTILITY FUNCTION TO RUN EXPERIMENT\n",
    "###############################################################################\n",
    "def run_experiment(variation_name, model_params, epochs=25, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model given by model_params, for 'epochs' epochs,\n",
    "    using MSE loss + Adam at LR=lr. Returns (model, run_info).\n",
    "    \"\"\"\n",
    "    model = get_model(**model_params).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # We'll store results here for logging/analysis\n",
    "    run_info = {\n",
    "        'variation': variation_name,\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'epoch_time': [],\n",
    "        # 'num_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        run_info['epoch'].append(epoch)\n",
    "        run_info['train_loss'].append(train_loss)\n",
    "        run_info['val_loss'].append(val_loss)\n",
    "        run_info['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"[{variation_name}] Epoch {epoch}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Final Test\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    run_info['test_loss'] = test_loss\n",
    "\n",
    "    # Additional metrics\n",
    "    preds, targets = predict(model, test_loader, device)\n",
    "    preds_np, targets_np = preds.numpy(), targets.numpy()\n",
    "    run_info['test_mae'] = mean_absolute_error(targets_np, preds_np)\n",
    "    run_info['test_r2']  = r2_score(targets_np, preds_np)\n",
    "\n",
    "    print(f\"[{variation_name}] FINAL TEST | \"\n",
    "          f\"MSE: {test_loss:.4f} | MAE: {run_info['test_mae']:.4f} | R^2: {run_info['test_r2']:.4f}\")\n",
    "\n",
    "    return model, run_info\n",
    "\n",
    "# src/train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_data in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass edge_attr to the model\n",
    "        out = model(\n",
    "            x=batch_data.x.float(),\n",
    "            edge_index=batch_data.edge_index,\n",
    "            batch=batch_data.batch,\n",
    "            edge_attr=batch_data.edge_attr.float()  # <-- Added\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        loss = criterion(out, batch_data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_data in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        out = model(\n",
    "            x=batch_data.x.float(),\n",
    "            edge_index=batch_data.edge_index,\n",
    "            batch=batch_data.batch,\n",
    "            edge_attr=batch_data.edge_attr.float()  # <-- Added\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        loss = criterion(out, batch_data.y.float())\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Return predictions and targets for analysis (plotting, etc).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    for batch_data in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        out = model(\n",
    "            x=batch_data.x.float(),\n",
    "            edge_index=batch_data.edge_index,\n",
    "            batch=batch_data.batch,\n",
    "            edge_attr=batch_data.edge_attr.float()  # <-- Added\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        all_preds.append(out.detach().cpu())\n",
    "        all_targets.append(batch_data.y.cpu())\n",
    "\n",
    "    return torch.cat(all_preds), torch.cat(all_targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GINE_V2_Dropout_LeakyReLU] Epoch 5/3000 | Train Loss: 2.4782 | Val Loss: 2.4598 | Time: 0.87s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 10/3000 | Train Loss: 2.3808 | Val Loss: 2.4162 | Time: 0.78s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 15/3000 | Train Loss: 2.2669 | Val Loss: 2.6009 | Time: 0.93s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 20/3000 | Train Loss: 2.1741 | Val Loss: 2.2120 | Time: 1.02s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 25/3000 | Train Loss: 2.0724 | Val Loss: 2.0570 | Time: 1.08s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 30/3000 | Train Loss: 2.0011 | Val Loss: 2.0009 | Time: 1.07s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 35/3000 | Train Loss: 1.9120 | Val Loss: 1.8161 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 40/3000 | Train Loss: 1.9117 | Val Loss: 1.7921 | Time: 1.50s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 45/3000 | Train Loss: 1.8958 | Val Loss: 2.1936 | Time: 0.95s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 50/3000 | Train Loss: 1.8443 | Val Loss: 1.8012 | Time: 0.87s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 55/3000 | Train Loss: 1.8319 | Val Loss: 1.7304 | Time: 0.92s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 60/3000 | Train Loss: 1.8079 | Val Loss: 1.7445 | Time: 0.89s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 65/3000 | Train Loss: 1.7558 | Val Loss: 1.7130 | Time: 0.90s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 70/3000 | Train Loss: 1.7619 | Val Loss: 2.0015 | Time: 0.94s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 75/3000 | Train Loss: 1.7394 | Val Loss: 1.7655 | Time: 0.91s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 80/3000 | Train Loss: 1.6866 | Val Loss: 1.7414 | Time: 0.91s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 85/3000 | Train Loss: 1.7630 | Val Loss: 1.7309 | Time: 1.05s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 90/3000 | Train Loss: 1.6928 | Val Loss: 1.7135 | Time: 0.98s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 95/3000 | Train Loss: 1.7027 | Val Loss: 1.7077 | Time: 1.37s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 100/3000 | Train Loss: 1.6960 | Val Loss: 1.6795 | Time: 0.96s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 105/3000 | Train Loss: 1.7132 | Val Loss: 1.7234 | Time: 0.98s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 110/3000 | Train Loss: 1.7157 | Val Loss: 1.8144 | Time: 1.00s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 115/3000 | Train Loss: 1.6840 | Val Loss: 1.6197 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 120/3000 | Train Loss: 1.6928 | Val Loss: 1.7514 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 125/3000 | Train Loss: 1.6520 | Val Loss: 1.6489 | Time: 1.55s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 130/3000 | Train Loss: 1.6593 | Val Loss: 1.6750 | Time: 1.62s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 135/3000 | Train Loss: 1.6443 | Val Loss: 1.7445 | Time: 1.55s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 140/3000 | Train Loss: 1.6318 | Val Loss: 1.7023 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 145/3000 | Train Loss: 1.6514 | Val Loss: 1.6464 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 150/3000 | Train Loss: 1.6404 | Val Loss: 1.6941 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 155/3000 | Train Loss: 1.6262 | Val Loss: 1.5669 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 160/3000 | Train Loss: 1.6355 | Val Loss: 1.6531 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 165/3000 | Train Loss: 1.6109 | Val Loss: 1.5621 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 170/3000 | Train Loss: 1.6076 | Val Loss: 1.6236 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 175/3000 | Train Loss: 1.6107 | Val Loss: 1.8985 | Time: 1.13s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 180/3000 | Train Loss: 1.6140 | Val Loss: 1.6498 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 185/3000 | Train Loss: 1.5915 | Val Loss: 1.5580 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 190/3000 | Train Loss: 1.5879 | Val Loss: 1.7160 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 195/3000 | Train Loss: 1.5809 | Val Loss: 1.6086 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 200/3000 | Train Loss: 1.5781 | Val Loss: 1.5772 | Time: 1.10s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 205/3000 | Train Loss: 1.5721 | Val Loss: 1.5612 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 210/3000 | Train Loss: 1.5998 | Val Loss: 1.5902 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 215/3000 | Train Loss: 1.5637 | Val Loss: 1.5643 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 220/3000 | Train Loss: 1.5730 | Val Loss: 1.6445 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 225/3000 | Train Loss: 1.5713 | Val Loss: 1.5748 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 230/3000 | Train Loss: 1.5570 | Val Loss: 1.5931 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 235/3000 | Train Loss: 1.5681 | Val Loss: 1.5453 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 240/3000 | Train Loss: 1.5682 | Val Loss: 1.6073 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 245/3000 | Train Loss: 1.5342 | Val Loss: 1.5964 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 250/3000 | Train Loss: 1.5347 | Val Loss: 1.5487 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 255/3000 | Train Loss: 1.5462 | Val Loss: 1.4912 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 260/3000 | Train Loss: 1.5188 | Val Loss: 1.5105 | Time: 1.14s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 265/3000 | Train Loss: 1.5336 | Val Loss: 1.5513 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 270/3000 | Train Loss: 1.5246 | Val Loss: 1.5053 | Time: 1.12s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 275/3000 | Train Loss: 1.5151 | Val Loss: 1.5223 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 280/3000 | Train Loss: 1.5077 | Val Loss: 1.5922 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 285/3000 | Train Loss: 1.5133 | Val Loss: 1.6417 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 290/3000 | Train Loss: 1.5032 | Val Loss: 1.6512 | Time: 1.12s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 295/3000 | Train Loss: 1.5084 | Val Loss: 1.5747 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 300/3000 | Train Loss: 1.5002 | Val Loss: 1.4937 | Time: 1.10s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 305/3000 | Train Loss: 1.5211 | Val Loss: 1.4803 | Time: 1.13s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 310/3000 | Train Loss: 1.5045 | Val Loss: 1.5871 | Time: 1.13s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 315/3000 | Train Loss: 1.4850 | Val Loss: 1.4745 | Time: 1.14s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 320/3000 | Train Loss: 1.4809 | Val Loss: 1.5171 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 325/3000 | Train Loss: 1.5329 | Val Loss: 1.4968 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 330/3000 | Train Loss: 1.4975 | Val Loss: 1.5758 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 335/3000 | Train Loss: 1.4983 | Val Loss: 1.4744 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 340/3000 | Train Loss: 1.4793 | Val Loss: 1.4456 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 345/3000 | Train Loss: 1.4812 | Val Loss: 1.4829 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 350/3000 | Train Loss: 1.4787 | Val Loss: 1.4573 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 355/3000 | Train Loss: 1.4839 | Val Loss: 1.5189 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 360/3000 | Train Loss: 1.4861 | Val Loss: 1.4830 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 365/3000 | Train Loss: 1.4636 | Val Loss: 1.5434 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 370/3000 | Train Loss: 1.4687 | Val Loss: 1.5291 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 375/3000 | Train Loss: 1.4604 | Val Loss: 1.4827 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 380/3000 | Train Loss: 1.4708 | Val Loss: 1.4750 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 385/3000 | Train Loss: 1.4587 | Val Loss: 1.4628 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 390/3000 | Train Loss: 1.4760 | Val Loss: 1.4485 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 395/3000 | Train Loss: 1.4696 | Val Loss: 1.5366 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 400/3000 | Train Loss: 1.4648 | Val Loss: 1.4554 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 405/3000 | Train Loss: 1.4754 | Val Loss: 1.5240 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 410/3000 | Train Loss: 1.4540 | Val Loss: 1.4940 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 415/3000 | Train Loss: 1.4494 | Val Loss: 1.4508 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 420/3000 | Train Loss: 1.4454 | Val Loss: 1.4601 | Time: 1.13s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 425/3000 | Train Loss: 1.4792 | Val Loss: 1.4353 | Time: 1.14s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 430/3000 | Train Loss: 1.4550 | Val Loss: 1.4442 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 435/3000 | Train Loss: 1.4358 | Val Loss: 1.5218 | Time: 1.14s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 440/3000 | Train Loss: 1.4452 | Val Loss: 1.5676 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 445/3000 | Train Loss: 1.4604 | Val Loss: 1.4569 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 450/3000 | Train Loss: 1.4518 | Val Loss: 1.4255 | Time: 1.41s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 455/3000 | Train Loss: 1.4402 | Val Loss: 1.5649 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 460/3000 | Train Loss: 1.4407 | Val Loss: 1.4585 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 465/3000 | Train Loss: 1.4464 | Val Loss: 1.4183 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 470/3000 | Train Loss: 1.4356 | Val Loss: 1.4313 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 475/3000 | Train Loss: 1.4353 | Val Loss: 1.5034 | Time: 1.44s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 480/3000 | Train Loss: 1.4481 | Val Loss: 1.4215 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 485/3000 | Train Loss: 1.4447 | Val Loss: 1.4015 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 490/3000 | Train Loss: 1.4268 | Val Loss: 1.4429 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 495/3000 | Train Loss: 1.4391 | Val Loss: 1.5484 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 500/3000 | Train Loss: 1.4269 | Val Loss: 1.4886 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 505/3000 | Train Loss: 1.4323 | Val Loss: 1.4603 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 510/3000 | Train Loss: 1.4141 | Val Loss: 1.4090 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 515/3000 | Train Loss: 1.4342 | Val Loss: 1.4418 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 520/3000 | Train Loss: 1.4326 | Val Loss: 1.4092 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 525/3000 | Train Loss: 1.4447 | Val Loss: 1.3860 | Time: 1.14s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 530/3000 | Train Loss: 1.4453 | Val Loss: 1.5168 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 535/3000 | Train Loss: 1.4351 | Val Loss: 1.4258 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 540/3000 | Train Loss: 1.4175 | Val Loss: 1.4016 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 545/3000 | Train Loss: 1.4099 | Val Loss: 1.4844 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 550/3000 | Train Loss: 1.4286 | Val Loss: 1.4402 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 555/3000 | Train Loss: 1.4117 | Val Loss: 1.4682 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 560/3000 | Train Loss: 1.4234 | Val Loss: 1.4022 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 565/3000 | Train Loss: 1.4066 | Val Loss: 1.4056 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 570/3000 | Train Loss: 1.4350 | Val Loss: 1.4392 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 575/3000 | Train Loss: 1.4161 | Val Loss: 1.4193 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 580/3000 | Train Loss: 1.4182 | Val Loss: 1.4616 | Time: 1.50s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 585/3000 | Train Loss: 1.4266 | Val Loss: 1.4276 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 590/3000 | Train Loss: 1.3998 | Val Loss: 1.4730 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 595/3000 | Train Loss: 1.4094 | Val Loss: 1.4291 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 600/3000 | Train Loss: 1.4294 | Val Loss: 1.3927 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 605/3000 | Train Loss: 1.4039 | Val Loss: 1.3986 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 610/3000 | Train Loss: 1.4196 | Val Loss: 1.4548 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 615/3000 | Train Loss: 1.4087 | Val Loss: 1.4553 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 620/3000 | Train Loss: 1.4145 | Val Loss: 1.4633 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 625/3000 | Train Loss: 1.4195 | Val Loss: 1.4697 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 630/3000 | Train Loss: 1.4144 | Val Loss: 1.3792 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 635/3000 | Train Loss: 1.4197 | Val Loss: 1.4787 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 640/3000 | Train Loss: 1.4085 | Val Loss: 1.4464 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 645/3000 | Train Loss: 1.3966 | Val Loss: 1.4149 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 650/3000 | Train Loss: 1.4115 | Val Loss: 1.4677 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 655/3000 | Train Loss: 1.3984 | Val Loss: 1.5226 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 660/3000 | Train Loss: 1.4027 | Val Loss: 1.3761 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 665/3000 | Train Loss: 1.4348 | Val Loss: 1.4397 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 670/3000 | Train Loss: 1.4113 | Val Loss: 1.5707 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 675/3000 | Train Loss: 1.4182 | Val Loss: 1.4417 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 680/3000 | Train Loss: 1.4362 | Val Loss: 1.4618 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 685/3000 | Train Loss: 1.4037 | Val Loss: 1.4079 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 690/3000 | Train Loss: 1.3915 | Val Loss: 1.4005 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 695/3000 | Train Loss: 1.3931 | Val Loss: 1.5489 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 700/3000 | Train Loss: 1.3837 | Val Loss: 1.3708 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 705/3000 | Train Loss: 1.4013 | Val Loss: 1.3564 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 710/3000 | Train Loss: 1.4352 | Val Loss: 1.3879 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 715/3000 | Train Loss: 1.3834 | Val Loss: 1.3814 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 720/3000 | Train Loss: 1.3938 | Val Loss: 1.3959 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 725/3000 | Train Loss: 1.4109 | Val Loss: 1.3785 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 730/3000 | Train Loss: 1.3821 | Val Loss: 1.3773 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 735/3000 | Train Loss: 1.3978 | Val Loss: 1.4315 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 740/3000 | Train Loss: 1.3778 | Val Loss: 1.3591 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 745/3000 | Train Loss: 1.3836 | Val Loss: 1.4122 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 750/3000 | Train Loss: 1.4222 | Val Loss: 1.4487 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 755/3000 | Train Loss: 1.3779 | Val Loss: 1.3541 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 760/3000 | Train Loss: 1.3816 | Val Loss: 1.3901 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 765/3000 | Train Loss: 1.4037 | Val Loss: 1.4671 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 770/3000 | Train Loss: 1.3888 | Val Loss: 1.3992 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 775/3000 | Train Loss: 1.4009 | Val Loss: 1.3926 | Time: 2.47s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 780/3000 | Train Loss: 1.3980 | Val Loss: 1.3450 | Time: 1.41s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 785/3000 | Train Loss: 1.3898 | Val Loss: 1.5661 | Time: 1.48s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 790/3000 | Train Loss: 1.3720 | Val Loss: 1.4148 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 795/3000 | Train Loss: 1.3887 | Val Loss: 1.4060 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 800/3000 | Train Loss: 1.3756 | Val Loss: 1.4407 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 805/3000 | Train Loss: 1.3748 | Val Loss: 1.3798 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 810/3000 | Train Loss: 1.3848 | Val Loss: 1.5396 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 815/3000 | Train Loss: 1.3699 | Val Loss: 1.3473 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 820/3000 | Train Loss: 1.4045 | Val Loss: 1.5057 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 825/3000 | Train Loss: 1.3752 | Val Loss: 1.3522 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 830/3000 | Train Loss: 1.3683 | Val Loss: 1.4355 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 835/3000 | Train Loss: 1.3746 | Val Loss: 1.3647 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 840/3000 | Train Loss: 1.3753 | Val Loss: 1.3866 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 845/3000 | Train Loss: 1.3739 | Val Loss: 1.4749 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 850/3000 | Train Loss: 1.3892 | Val Loss: 1.3516 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 855/3000 | Train Loss: 1.3743 | Val Loss: 1.4066 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 860/3000 | Train Loss: 1.3723 | Val Loss: 1.3755 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 865/3000 | Train Loss: 1.3642 | Val Loss: 1.3786 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 870/3000 | Train Loss: 1.3616 | Val Loss: 1.3560 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 875/3000 | Train Loss: 1.3704 | Val Loss: 1.3879 | Time: 1.45s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 880/3000 | Train Loss: 1.3924 | Val Loss: 1.4056 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 885/3000 | Train Loss: 1.3723 | Val Loss: 1.3686 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 890/3000 | Train Loss: 1.3805 | Val Loss: 1.4669 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 895/3000 | Train Loss: 1.3763 | Val Loss: 1.3553 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 900/3000 | Train Loss: 1.3648 | Val Loss: 1.3667 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 905/3000 | Train Loss: 1.3730 | Val Loss: 1.3366 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 910/3000 | Train Loss: 1.3645 | Val Loss: 1.3496 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 915/3000 | Train Loss: 1.3828 | Val Loss: 1.4351 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 920/3000 | Train Loss: 1.3694 | Val Loss: 1.3537 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 925/3000 | Train Loss: 1.3849 | Val Loss: 1.3691 | Time: 1.37s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 930/3000 | Train Loss: 1.3770 | Val Loss: 1.3639 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 935/3000 | Train Loss: 1.3543 | Val Loss: 1.3849 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 940/3000 | Train Loss: 1.3770 | Val Loss: 1.3716 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 945/3000 | Train Loss: 1.3963 | Val Loss: 1.3644 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 950/3000 | Train Loss: 1.3626 | Val Loss: 1.3624 | Time: 1.53s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 955/3000 | Train Loss: 1.3607 | Val Loss: 1.3391 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 960/3000 | Train Loss: 1.3749 | Val Loss: 1.4995 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 965/3000 | Train Loss: 1.3594 | Val Loss: 1.4451 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 970/3000 | Train Loss: 1.3549 | Val Loss: 1.3536 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 975/3000 | Train Loss: 1.3674 | Val Loss: 1.3717 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 980/3000 | Train Loss: 1.3642 | Val Loss: 1.4359 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 985/3000 | Train Loss: 1.3748 | Val Loss: 1.4685 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 990/3000 | Train Loss: 1.3560 | Val Loss: 1.3952 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 995/3000 | Train Loss: 1.3717 | Val Loss: 1.4052 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1000/3000 | Train Loss: 1.3803 | Val Loss: 1.4085 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1005/3000 | Train Loss: 1.3588 | Val Loss: 1.3342 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1010/3000 | Train Loss: 1.3715 | Val Loss: 1.3689 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1015/3000 | Train Loss: 1.3546 | Val Loss: 1.4169 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1020/3000 | Train Loss: 1.3581 | Val Loss: 1.3273 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1025/3000 | Train Loss: 1.3707 | Val Loss: 1.3469 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1030/3000 | Train Loss: 1.3680 | Val Loss: 1.3517 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1035/3000 | Train Loss: 1.3576 | Val Loss: 1.3855 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1040/3000 | Train Loss: 1.3748 | Val Loss: 1.4350 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1045/3000 | Train Loss: 1.3451 | Val Loss: 1.4116 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1050/3000 | Train Loss: 1.3575 | Val Loss: 1.3543 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1055/3000 | Train Loss: 1.3414 | Val Loss: 1.4093 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1060/3000 | Train Loss: 1.3686 | Val Loss: 1.3357 | Time: 1.42s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1065/3000 | Train Loss: 1.3508 | Val Loss: 1.3336 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1070/3000 | Train Loss: 1.3482 | Val Loss: 1.3336 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1075/3000 | Train Loss: 1.3589 | Val Loss: 1.3554 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1080/3000 | Train Loss: 1.3589 | Val Loss: 1.3316 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1085/3000 | Train Loss: 1.3613 | Val Loss: 1.4156 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1090/3000 | Train Loss: 1.3543 | Val Loss: 1.3462 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1095/3000 | Train Loss: 1.3393 | Val Loss: 1.3952 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1100/3000 | Train Loss: 1.3820 | Val Loss: 1.4519 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1105/3000 | Train Loss: 1.3672 | Val Loss: 1.3560 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1110/3000 | Train Loss: 1.3466 | Val Loss: 1.3718 | Time: 1.52s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1115/3000 | Train Loss: 1.3390 | Val Loss: 1.4234 | Time: 1.55s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1120/3000 | Train Loss: 1.3584 | Val Loss: 1.3558 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1125/3000 | Train Loss: 1.3531 | Val Loss: 1.4356 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1130/3000 | Train Loss: 1.3467 | Val Loss: 1.3540 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1135/3000 | Train Loss: 1.3426 | Val Loss: 1.4559 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1140/3000 | Train Loss: 1.3533 | Val Loss: 1.3480 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1145/3000 | Train Loss: 1.3561 | Val Loss: 1.3218 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1150/3000 | Train Loss: 1.3468 | Val Loss: 1.3169 | Time: 1.45s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1155/3000 | Train Loss: 1.3600 | Val Loss: 1.4272 | Time: 1.69s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1160/3000 | Train Loss: 1.3517 | Val Loss: 1.3635 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1165/3000 | Train Loss: 1.3336 | Val Loss: 1.3659 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1170/3000 | Train Loss: 1.3520 | Val Loss: 1.4394 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1175/3000 | Train Loss: 1.3447 | Val Loss: 1.3248 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1180/3000 | Train Loss: 1.3419 | Val Loss: 1.3407 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1185/3000 | Train Loss: 1.3503 | Val Loss: 1.3926 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1190/3000 | Train Loss: 1.3490 | Val Loss: 1.3880 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1195/3000 | Train Loss: 1.3425 | Val Loss: 1.4484 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1200/3000 | Train Loss: 1.3182 | Val Loss: 1.3906 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1205/3000 | Train Loss: 1.3198 | Val Loss: 1.3913 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1210/3000 | Train Loss: 1.3411 | Val Loss: 1.4264 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1215/3000 | Train Loss: 1.3424 | Val Loss: 1.4003 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1220/3000 | Train Loss: 1.3414 | Val Loss: 1.3499 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1225/3000 | Train Loss: 1.3270 | Val Loss: 1.4370 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1230/3000 | Train Loss: 1.3328 | Val Loss: 1.3654 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1235/3000 | Train Loss: 1.3371 | Val Loss: 1.3378 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1240/3000 | Train Loss: 1.3361 | Val Loss: 1.4707 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1245/3000 | Train Loss: 1.3335 | Val Loss: 1.3595 | Time: 1.14s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1250/3000 | Train Loss: 1.3413 | Val Loss: 1.3328 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1255/3000 | Train Loss: 1.3531 | Val Loss: 1.3638 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1260/3000 | Train Loss: 1.3388 | Val Loss: 1.3564 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1265/3000 | Train Loss: 1.3439 | Val Loss: 1.3614 | Time: 1.59s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1270/3000 | Train Loss: 1.3442 | Val Loss: 1.3598 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1275/3000 | Train Loss: 1.3332 | Val Loss: 1.3123 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1280/3000 | Train Loss: 1.3358 | Val Loss: 1.3754 | Time: 1.43s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1285/3000 | Train Loss: 1.3169 | Val Loss: 1.3981 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1290/3000 | Train Loss: 1.3251 | Val Loss: 1.3519 | Time: 1.48s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1295/3000 | Train Loss: 1.3377 | Val Loss: 1.3715 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1300/3000 | Train Loss: 1.3479 | Val Loss: 1.3649 | Time: 1.49s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1305/3000 | Train Loss: 1.4211 | Val Loss: 1.4434 | Time: 1.43s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1310/3000 | Train Loss: 1.3281 | Val Loss: 1.3995 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1315/3000 | Train Loss: 1.3349 | Val Loss: 1.3499 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1320/3000 | Train Loss: 1.3237 | Val Loss: 1.3988 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1325/3000 | Train Loss: 1.3172 | Val Loss: 1.3183 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1330/3000 | Train Loss: 1.3291 | Val Loss: 1.4133 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1335/3000 | Train Loss: 1.3243 | Val Loss: 1.3246 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1340/3000 | Train Loss: 1.3446 | Val Loss: 1.3672 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1345/3000 | Train Loss: 1.3058 | Val Loss: 1.3489 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1350/3000 | Train Loss: 1.3122 | Val Loss: 1.3144 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1355/3000 | Train Loss: 1.3289 | Val Loss: 1.3135 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1360/3000 | Train Loss: 1.3349 | Val Loss: 1.3413 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1365/3000 | Train Loss: 1.3421 | Val Loss: 1.3086 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1370/3000 | Train Loss: 1.3269 | Val Loss: 1.3625 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1375/3000 | Train Loss: 1.3128 | Val Loss: 1.3477 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1380/3000 | Train Loss: 1.3131 | Val Loss: 1.2974 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1385/3000 | Train Loss: 1.3321 | Val Loss: 1.3417 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1390/3000 | Train Loss: 1.3417 | Val Loss: 1.3479 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1395/3000 | Train Loss: 1.3222 | Val Loss: 1.3279 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1400/3000 | Train Loss: 1.3079 | Val Loss: 1.3892 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1405/3000 | Train Loss: 1.3234 | Val Loss: 1.3492 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1410/3000 | Train Loss: 1.3272 | Val Loss: 1.3253 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1415/3000 | Train Loss: 1.3374 | Val Loss: 1.3754 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1420/3000 | Train Loss: 1.3172 | Val Loss: 1.3253 | Time: 1.49s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1425/3000 | Train Loss: 1.3194 | Val Loss: 1.3158 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1430/3000 | Train Loss: 1.3319 | Val Loss: 1.3092 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1435/3000 | Train Loss: 1.3148 | Val Loss: 1.3295 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1440/3000 | Train Loss: 1.3322 | Val Loss: 1.5197 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1445/3000 | Train Loss: 1.3367 | Val Loss: 1.3268 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1450/3000 | Train Loss: 1.3228 | Val Loss: 1.3308 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1455/3000 | Train Loss: 1.3267 | Val Loss: 1.3162 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1460/3000 | Train Loss: 1.3215 | Val Loss: 1.3148 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1465/3000 | Train Loss: 1.3078 | Val Loss: 1.5231 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1470/3000 | Train Loss: 1.3287 | Val Loss: 1.3761 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1475/3000 | Train Loss: 1.3328 | Val Loss: 1.4175 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1480/3000 | Train Loss: 1.3148 | Val Loss: 1.3602 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1485/3000 | Train Loss: 1.3039 | Val Loss: 1.4864 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1490/3000 | Train Loss: 1.3061 | Val Loss: 1.2911 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1495/3000 | Train Loss: 1.3095 | Val Loss: 1.3270 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1500/3000 | Train Loss: 1.3082 | Val Loss: 1.3232 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1505/3000 | Train Loss: 1.3135 | Val Loss: 1.3474 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1510/3000 | Train Loss: 1.3390 | Val Loss: 1.3279 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1515/3000 | Train Loss: 1.3166 | Val Loss: 1.3220 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1520/3000 | Train Loss: 1.3249 | Val Loss: 1.3104 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1525/3000 | Train Loss: 1.2907 | Val Loss: 1.3346 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1530/3000 | Train Loss: 1.3209 | Val Loss: 1.3222 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1535/3000 | Train Loss: 1.3162 | Val Loss: 1.3295 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1540/3000 | Train Loss: 1.3012 | Val Loss: 1.3751 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1545/3000 | Train Loss: 1.2981 | Val Loss: 1.3039 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1550/3000 | Train Loss: 1.3157 | Val Loss: 1.4998 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1555/3000 | Train Loss: 1.3108 | Val Loss: 1.4148 | Time: 1.42s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1560/3000 | Train Loss: 1.3270 | Val Loss: 1.3591 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1565/3000 | Train Loss: 1.2993 | Val Loss: 1.3750 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1570/3000 | Train Loss: 1.3009 | Val Loss: 1.2991 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1575/3000 | Train Loss: 1.3037 | Val Loss: 1.2847 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1580/3000 | Train Loss: 1.3202 | Val Loss: 1.3643 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1585/3000 | Train Loss: 1.3128 | Val Loss: 1.3322 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1590/3000 | Train Loss: 1.3152 | Val Loss: 1.3020 | Time: 1.56s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1595/3000 | Train Loss: 1.3138 | Val Loss: 1.3465 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1600/3000 | Train Loss: 1.3108 | Val Loss: 1.3013 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1605/3000 | Train Loss: 1.2853 | Val Loss: 1.2991 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1610/3000 | Train Loss: 1.3130 | Val Loss: 1.3701 | Time: 1.37s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1615/3000 | Train Loss: 1.2897 | Val Loss: 1.3474 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1620/3000 | Train Loss: 1.3020 | Val Loss: 1.3028 | Time: 1.37s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1625/3000 | Train Loss: 1.3069 | Val Loss: 1.3595 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1630/3000 | Train Loss: 1.2954 | Val Loss: 1.3235 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1635/3000 | Train Loss: 1.2900 | Val Loss: 1.3159 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1640/3000 | Train Loss: 1.3063 | Val Loss: 1.3281 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1645/3000 | Train Loss: 1.2852 | Val Loss: 1.4372 | Time: 1.40s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1650/3000 | Train Loss: 1.3079 | Val Loss: 1.3405 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1655/3000 | Train Loss: 1.3031 | Val Loss: 1.3051 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1660/3000 | Train Loss: 1.3106 | Val Loss: 1.3282 | Time: 1.76s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1665/3000 | Train Loss: 1.3130 | Val Loss: 1.3993 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1670/3000 | Train Loss: 1.3024 | Val Loss: 1.3474 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1675/3000 | Train Loss: 1.3111 | Val Loss: 1.3025 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1680/3000 | Train Loss: 1.2919 | Val Loss: 1.3134 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1685/3000 | Train Loss: 1.3104 | Val Loss: 1.3284 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1690/3000 | Train Loss: 1.2986 | Val Loss: 1.3874 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1695/3000 | Train Loss: 1.2939 | Val Loss: 1.3255 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1700/3000 | Train Loss: 1.3154 | Val Loss: 1.3194 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1705/3000 | Train Loss: 1.2950 | Val Loss: 1.3079 | Time: 1.39s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1710/3000 | Train Loss: 1.3063 | Val Loss: 1.3366 | Time: 2.04s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1715/3000 | Train Loss: 1.2868 | Val Loss: 1.3697 | Time: 1.98s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1720/3000 | Train Loss: 1.2878 | Val Loss: 1.3558 | Time: 1.51s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1725/3000 | Train Loss: 1.3164 | Val Loss: 1.3504 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1730/3000 | Train Loss: 1.3154 | Val Loss: 1.3405 | Time: 1.39s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1735/3000 | Train Loss: 1.2932 | Val Loss: 1.4856 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1740/3000 | Train Loss: 1.3006 | Val Loss: 1.2737 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1745/3000 | Train Loss: 1.2919 | Val Loss: 1.3216 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1750/3000 | Train Loss: 1.3031 | Val Loss: 1.2966 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1755/3000 | Train Loss: 1.3126 | Val Loss: 1.2987 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1760/3000 | Train Loss: 1.2920 | Val Loss: 1.4143 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1765/3000 | Train Loss: 1.3070 | Val Loss: 1.2937 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1770/3000 | Train Loss: 1.2955 | Val Loss: 1.3305 | Time: 1.45s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1775/3000 | Train Loss: 1.2851 | Val Loss: 1.3131 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1780/3000 | Train Loss: 1.3093 | Val Loss: 1.3090 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1785/3000 | Train Loss: 1.2881 | Val Loss: 1.3126 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1790/3000 | Train Loss: 1.2950 | Val Loss: 1.3373 | Time: 1.46s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1795/3000 | Train Loss: 1.2918 | Val Loss: 1.3124 | Time: 1.45s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1800/3000 | Train Loss: 1.2742 | Val Loss: 1.3140 | Time: 1.40s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1805/3000 | Train Loss: 1.3027 | Val Loss: 1.3502 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1810/3000 | Train Loss: 1.2881 | Val Loss: 1.3391 | Time: 1.39s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1815/3000 | Train Loss: 1.3076 | Val Loss: 1.3227 | Time: 1.41s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1820/3000 | Train Loss: 1.3258 | Val Loss: 1.2885 | Time: 1.47s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1825/3000 | Train Loss: 1.3183 | Val Loss: 1.3190 | Time: 1.38s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1830/3000 | Train Loss: 1.2790 | Val Loss: 1.3910 | Time: 1.45s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1835/3000 | Train Loss: 1.2982 | Val Loss: 1.2657 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1840/3000 | Train Loss: 1.3062 | Val Loss: 1.3342 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1845/3000 | Train Loss: 1.2988 | Val Loss: 1.4280 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1850/3000 | Train Loss: 1.2910 | Val Loss: 1.3268 | Time: 1.38s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1855/3000 | Train Loss: 1.2861 | Val Loss: 1.3129 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1860/3000 | Train Loss: 1.2889 | Val Loss: 1.2834 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1865/3000 | Train Loss: 1.2843 | Val Loss: 1.5406 | Time: 1.39s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1870/3000 | Train Loss: 1.2874 | Val Loss: 1.2749 | Time: 1.89s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1875/3000 | Train Loss: 1.2904 | Val Loss: 1.2872 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1880/3000 | Train Loss: 1.2741 | Val Loss: 1.3024 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1885/3000 | Train Loss: 1.2786 | Val Loss: 1.2818 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1890/3000 | Train Loss: 1.2919 | Val Loss: 1.3564 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1895/3000 | Train Loss: 1.2902 | Val Loss: 1.3448 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1900/3000 | Train Loss: 1.2783 | Val Loss: 1.2823 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1905/3000 | Train Loss: 1.2821 | Val Loss: 1.2689 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1910/3000 | Train Loss: 1.2819 | Val Loss: 1.3028 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1915/3000 | Train Loss: 1.2809 | Val Loss: 1.3678 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1920/3000 | Train Loss: 1.2719 | Val Loss: 1.3005 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1925/3000 | Train Loss: 1.2972 | Val Loss: 1.2808 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1930/3000 | Train Loss: 1.2760 | Val Loss: 1.3402 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1935/3000 | Train Loss: 1.2984 | Val Loss: 1.2922 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1940/3000 | Train Loss: 1.2831 | Val Loss: 1.2974 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1945/3000 | Train Loss: 1.2771 | Val Loss: 1.3308 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1950/3000 | Train Loss: 1.2991 | Val Loss: 1.3101 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1955/3000 | Train Loss: 1.2929 | Val Loss: 1.2869 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1960/3000 | Train Loss: 1.2790 | Val Loss: 1.4203 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1965/3000 | Train Loss: 1.2928 | Val Loss: 1.2875 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1970/3000 | Train Loss: 1.2638 | Val Loss: 1.2677 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1975/3000 | Train Loss: 1.2664 | Val Loss: 1.3051 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1980/3000 | Train Loss: 1.2877 | Val Loss: 1.2948 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1985/3000 | Train Loss: 1.2999 | Val Loss: 1.2817 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1990/3000 | Train Loss: 1.2706 | Val Loss: 1.2668 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 1995/3000 | Train Loss: 1.2889 | Val Loss: 1.4911 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2000/3000 | Train Loss: 1.2802 | Val Loss: 1.2829 | Time: 1.61s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2005/3000 | Train Loss: 1.2794 | Val Loss: 1.3322 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2010/3000 | Train Loss: 1.2914 | Val Loss: 1.3009 | Time: 1.40s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2015/3000 | Train Loss: 1.2903 | Val Loss: 1.2933 | Time: 2.02s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2020/3000 | Train Loss: 1.2894 | Val Loss: 1.2906 | Time: 1.43s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2025/3000 | Train Loss: 1.2816 | Val Loss: 1.2820 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2030/3000 | Train Loss: 1.2891 | Val Loss: 1.3025 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2035/3000 | Train Loss: 1.2705 | Val Loss: 1.2697 | Time: 1.42s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2040/3000 | Train Loss: 1.2831 | Val Loss: 1.2823 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2045/3000 | Train Loss: 1.2746 | Val Loss: 1.3292 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2050/3000 | Train Loss: 1.2822 | Val Loss: 1.3190 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2055/3000 | Train Loss: 1.2920 | Val Loss: 1.3237 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2060/3000 | Train Loss: 1.2841 | Val Loss: 1.2893 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2065/3000 | Train Loss: 1.2965 | Val Loss: 1.3363 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2070/3000 | Train Loss: 1.2919 | Val Loss: 1.2732 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2075/3000 | Train Loss: 1.2661 | Val Loss: 1.3109 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2080/3000 | Train Loss: 1.2754 | Val Loss: 1.2929 | Time: 1.37s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2085/3000 | Train Loss: 1.2920 | Val Loss: 1.2651 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2090/3000 | Train Loss: 1.2789 | Val Loss: 1.3037 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2095/3000 | Train Loss: 1.2873 | Val Loss: 1.2646 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2100/3000 | Train Loss: 1.2799 | Val Loss: 1.3011 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2105/3000 | Train Loss: 1.2956 | Val Loss: 1.3316 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2110/3000 | Train Loss: 1.2786 | Val Loss: 1.2802 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2115/3000 | Train Loss: 1.2820 | Val Loss: 1.4715 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2120/3000 | Train Loss: 1.2956 | Val Loss: 1.3733 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2125/3000 | Train Loss: 1.2711 | Val Loss: 1.2817 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2130/3000 | Train Loss: 1.2644 | Val Loss: 1.2936 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2135/3000 | Train Loss: 1.2860 | Val Loss: 1.3053 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2140/3000 | Train Loss: 1.2934 | Val Loss: 1.3347 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2145/3000 | Train Loss: 1.2876 | Val Loss: 1.4177 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2150/3000 | Train Loss: 1.2702 | Val Loss: 1.3624 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2155/3000 | Train Loss: 1.3132 | Val Loss: 1.2693 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2160/3000 | Train Loss: 1.2966 | Val Loss: 1.2926 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2165/3000 | Train Loss: 1.2835 | Val Loss: 1.3011 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2170/3000 | Train Loss: 1.2896 | Val Loss: 1.2552 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2175/3000 | Train Loss: 1.2723 | Val Loss: 1.2578 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2180/3000 | Train Loss: 1.2799 | Val Loss: 1.3197 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2185/3000 | Train Loss: 1.2735 | Val Loss: 1.3083 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2190/3000 | Train Loss: 1.2691 | Val Loss: 1.3013 | Time: 1.44s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2195/3000 | Train Loss: 1.2799 | Val Loss: 1.2472 | Time: 1.37s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2200/3000 | Train Loss: 1.2755 | Val Loss: 1.2757 | Time: 1.97s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2205/3000 | Train Loss: 1.2633 | Val Loss: 1.2638 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2210/3000 | Train Loss: 1.2761 | Val Loss: 1.2593 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2215/3000 | Train Loss: 1.2574 | Val Loss: 1.2512 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2220/3000 | Train Loss: 1.2805 | Val Loss: 1.2839 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2225/3000 | Train Loss: 1.2648 | Val Loss: 1.4434 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2230/3000 | Train Loss: 1.2741 | Val Loss: 1.3033 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2235/3000 | Train Loss: 1.3071 | Val Loss: 1.2851 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2240/3000 | Train Loss: 1.2980 | Val Loss: 1.3215 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2245/3000 | Train Loss: 1.2719 | Val Loss: 1.3077 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2250/3000 | Train Loss: 1.2737 | Val Loss: 1.3060 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2255/3000 | Train Loss: 1.2790 | Val Loss: 1.2632 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2260/3000 | Train Loss: 1.2609 | Val Loss: 1.3217 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2265/3000 | Train Loss: 1.2670 | Val Loss: 1.3073 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2270/3000 | Train Loss: 1.2714 | Val Loss: 1.3233 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2275/3000 | Train Loss: 1.2956 | Val Loss: 1.2810 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2280/3000 | Train Loss: 1.2735 | Val Loss: 1.3679 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2285/3000 | Train Loss: 1.2613 | Val Loss: 1.2557 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2290/3000 | Train Loss: 1.2623 | Val Loss: 1.2842 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2295/3000 | Train Loss: 1.2623 | Val Loss: 1.3326 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2300/3000 | Train Loss: 1.2690 | Val Loss: 1.3687 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2305/3000 | Train Loss: 1.2813 | Val Loss: 1.3252 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2310/3000 | Train Loss: 1.2604 | Val Loss: 1.2745 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2315/3000 | Train Loss: 1.2948 | Val Loss: 1.3211 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2320/3000 | Train Loss: 1.2579 | Val Loss: 1.3462 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2325/3000 | Train Loss: 1.2557 | Val Loss: 1.2684 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2330/3000 | Train Loss: 1.2792 | Val Loss: 1.3302 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2335/3000 | Train Loss: 1.2580 | Val Loss: 1.2940 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2340/3000 | Train Loss: 1.2761 | Val Loss: 1.2546 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2345/3000 | Train Loss: 1.2508 | Val Loss: 1.2948 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2350/3000 | Train Loss: 1.2650 | Val Loss: 1.3002 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2355/3000 | Train Loss: 1.2650 | Val Loss: 1.2673 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2360/3000 | Train Loss: 1.2657 | Val Loss: 1.2957 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2365/3000 | Train Loss: 1.2702 | Val Loss: 1.3188 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2370/3000 | Train Loss: 1.2728 | Val Loss: 1.2834 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2375/3000 | Train Loss: 1.2867 | Val Loss: 1.2988 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2380/3000 | Train Loss: 1.2631 | Val Loss: 1.2822 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2385/3000 | Train Loss: 1.2667 | Val Loss: 1.2422 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2390/3000 | Train Loss: 1.2809 | Val Loss: 1.4047 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2395/3000 | Train Loss: 1.2696 | Val Loss: 1.3415 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2400/3000 | Train Loss: 1.2917 | Val Loss: 1.2990 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2405/3000 | Train Loss: 1.2686 | Val Loss: 1.2817 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2410/3000 | Train Loss: 1.2530 | Val Loss: 1.2610 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2415/3000 | Train Loss: 1.2824 | Val Loss: 1.3996 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2420/3000 | Train Loss: 1.2789 | Val Loss: 1.3107 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2425/3000 | Train Loss: 1.2646 | Val Loss: 1.2926 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2430/3000 | Train Loss: 1.2596 | Val Loss: 1.3713 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2435/3000 | Train Loss: 1.2610 | Val Loss: 1.3001 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2440/3000 | Train Loss: 1.2601 | Val Loss: 1.2773 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2445/3000 | Train Loss: 1.2586 | Val Loss: 1.2573 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2450/3000 | Train Loss: 1.2773 | Val Loss: 1.2640 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2455/3000 | Train Loss: 1.2584 | Val Loss: 1.2764 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2460/3000 | Train Loss: 1.2738 | Val Loss: 1.3474 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2465/3000 | Train Loss: 1.2617 | Val Loss: 1.2772 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2470/3000 | Train Loss: 1.2798 | Val Loss: 1.2847 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2475/3000 | Train Loss: 1.2470 | Val Loss: 1.3197 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2480/3000 | Train Loss: 1.2764 | Val Loss: 1.3165 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2485/3000 | Train Loss: 1.2474 | Val Loss: 1.2990 | Time: 1.23s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2490/3000 | Train Loss: 1.2705 | Val Loss: 1.3254 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2495/3000 | Train Loss: 1.2635 | Val Loss: 1.2679 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2500/3000 | Train Loss: 1.2621 | Val Loss: 1.2554 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2505/3000 | Train Loss: 1.2609 | Val Loss: 1.2363 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2510/3000 | Train Loss: 1.2614 | Val Loss: 1.2920 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2515/3000 | Train Loss: 1.2695 | Val Loss: 1.3214 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2520/3000 | Train Loss: 1.2563 | Val Loss: 1.3174 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2525/3000 | Train Loss: 1.2700 | Val Loss: 1.2905 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2530/3000 | Train Loss: 1.2648 | Val Loss: 1.3183 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2535/3000 | Train Loss: 1.2657 | Val Loss: 1.2701 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2540/3000 | Train Loss: 1.2774 | Val Loss: 1.3652 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2545/3000 | Train Loss: 1.2603 | Val Loss: 1.2645 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2550/3000 | Train Loss: 1.2692 | Val Loss: 1.2629 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2555/3000 | Train Loss: 1.2717 | Val Loss: 1.2776 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2560/3000 | Train Loss: 1.2866 | Val Loss: 1.2907 | Time: 1.24s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2565/3000 | Train Loss: 1.2956 | Val Loss: 1.2871 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2570/3000 | Train Loss: 1.2692 | Val Loss: 1.2603 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2575/3000 | Train Loss: 1.2528 | Val Loss: 1.3276 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2580/3000 | Train Loss: 1.2630 | Val Loss: 1.2632 | Time: 1.39s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2585/3000 | Train Loss: 1.2651 | Val Loss: 1.2862 | Time: 1.44s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2590/3000 | Train Loss: 1.2723 | Val Loss: 1.2662 | Time: 1.41s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2595/3000 | Train Loss: 1.2810 | Val Loss: 1.3676 | Time: 3.92s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2600/3000 | Train Loss: 1.2654 | Val Loss: 1.2575 | Time: 1.93s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2605/3000 | Train Loss: 1.2675 | Val Loss: 1.3501 | Time: 1.50s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2610/3000 | Train Loss: 1.2593 | Val Loss: 1.3310 | Time: 1.77s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2615/3000 | Train Loss: 1.2746 | Val Loss: 1.2755 | Time: 1.47s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2620/3000 | Train Loss: 1.2393 | Val Loss: 1.2828 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2625/3000 | Train Loss: 1.2562 | Val Loss: 1.2994 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2630/3000 | Train Loss: 1.2790 | Val Loss: 1.2565 | Time: 3.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2635/3000 | Train Loss: 1.2609 | Val Loss: 1.3587 | Time: 2.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2640/3000 | Train Loss: 1.2575 | Val Loss: 1.2708 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2645/3000 | Train Loss: 1.2451 | Val Loss: 1.3066 | Time: 1.44s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2650/3000 | Train Loss: 1.2744 | Val Loss: 1.2871 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2655/3000 | Train Loss: 1.2597 | Val Loss: 1.2722 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2660/3000 | Train Loss: 1.2598 | Val Loss: 1.3543 | Time: 1.38s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2665/3000 | Train Loss: 1.2503 | Val Loss: 1.2594 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2670/3000 | Train Loss: 1.2604 | Val Loss: 1.2820 | Time: 1.32s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2675/3000 | Train Loss: 1.2581 | Val Loss: 1.3139 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2680/3000 | Train Loss: 1.2766 | Val Loss: 1.2556 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2685/3000 | Train Loss: 1.2613 | Val Loss: 1.3168 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2690/3000 | Train Loss: 1.2642 | Val Loss: 1.3054 | Time: 1.42s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2695/3000 | Train Loss: 1.2537 | Val Loss: 1.2913 | Time: 1.48s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2700/3000 | Train Loss: 1.2614 | Val Loss: 1.3213 | Time: 1.70s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2705/3000 | Train Loss: 1.2541 | Val Loss: 1.2491 | Time: 1.42s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2710/3000 | Train Loss: 1.2741 | Val Loss: 1.3019 | Time: 1.88s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2715/3000 | Train Loss: 1.2706 | Val Loss: 1.3668 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2720/3000 | Train Loss: 1.2556 | Val Loss: 1.2560 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2725/3000 | Train Loss: 1.2472 | Val Loss: 1.3147 | Time: 1.16s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2730/3000 | Train Loss: 1.2654 | Val Loss: 1.2769 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2735/3000 | Train Loss: 1.2518 | Val Loss: 1.4081 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2740/3000 | Train Loss: 1.2458 | Val Loss: 1.2257 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2745/3000 | Train Loss: 1.2549 | Val Loss: 1.3057 | Time: 1.39s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2750/3000 | Train Loss: 1.2574 | Val Loss: 1.2936 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2755/3000 | Train Loss: 1.2459 | Val Loss: 1.2574 | Time: 1.20s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2760/3000 | Train Loss: 1.2644 | Val Loss: 1.2648 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2765/3000 | Train Loss: 1.2652 | Val Loss: 1.2758 | Time: 1.59s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2770/3000 | Train Loss: 1.2643 | Val Loss: 1.5819 | Time: 1.75s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2775/3000 | Train Loss: 1.2472 | Val Loss: 1.2777 | Time: 1.27s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2780/3000 | Train Loss: 1.2471 | Val Loss: 1.2586 | Time: 1.35s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2785/3000 | Train Loss: 1.2551 | Val Loss: 1.2297 | Time: 1.37s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2790/3000 | Train Loss: 1.2809 | Val Loss: 1.2594 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2795/3000 | Train Loss: 1.2549 | Val Loss: 1.2771 | Time: 1.47s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2800/3000 | Train Loss: 1.2516 | Val Loss: 1.2618 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2805/3000 | Train Loss: 1.2579 | Val Loss: 1.2835 | Time: 1.36s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2810/3000 | Train Loss: 1.2547 | Val Loss: 1.2510 | Time: 1.29s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2815/3000 | Train Loss: 1.2453 | Val Loss: 1.2417 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2820/3000 | Train Loss: 1.2577 | Val Loss: 1.3058 | Time: 1.46s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2825/3000 | Train Loss: 1.2546 | Val Loss: 1.4052 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2830/3000 | Train Loss: 1.2629 | Val Loss: 1.3132 | Time: 2.42s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2835/3000 | Train Loss: 1.3656 | Val Loss: 1.3164 | Time: 1.48s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2840/3000 | Train Loss: 1.2481 | Val Loss: 1.2412 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2845/3000 | Train Loss: 1.2534 | Val Loss: 1.4198 | Time: 1.71s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2850/3000 | Train Loss: 1.2671 | Val Loss: 1.2586 | Time: 1.34s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2855/3000 | Train Loss: 1.2867 | Val Loss: 1.3227 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2860/3000 | Train Loss: 1.2476 | Val Loss: 1.2922 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2865/3000 | Train Loss: 1.2640 | Val Loss: 1.2666 | Time: 1.13s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2870/3000 | Train Loss: 1.2688 | Val Loss: 1.2687 | Time: 1.15s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2875/3000 | Train Loss: 1.2561 | Val Loss: 1.6094 | Time: 1.17s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2880/3000 | Train Loss: 1.2725 | Val Loss: 1.2823 | Time: 1.22s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2885/3000 | Train Loss: 1.2610 | Val Loss: 1.2466 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2890/3000 | Train Loss: 1.2459 | Val Loss: 1.3343 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2895/3000 | Train Loss: 1.2468 | Val Loss: 1.2804 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2900/3000 | Train Loss: 1.2901 | Val Loss: 1.3694 | Time: 2.46s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2905/3000 | Train Loss: 1.2394 | Val Loss: 1.3003 | Time: 2.14s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2910/3000 | Train Loss: 1.2524 | Val Loss: 1.3333 | Time: 1.78s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2915/3000 | Train Loss: 1.2509 | Val Loss: 1.2596 | Time: 1.44s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2920/3000 | Train Loss: 1.2480 | Val Loss: 1.3163 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2925/3000 | Train Loss: 1.2616 | Val Loss: 1.2369 | Time: 1.31s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2930/3000 | Train Loss: 1.2477 | Val Loss: 1.2628 | Time: 1.25s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2935/3000 | Train Loss: 1.2486 | Val Loss: 1.2884 | Time: 1.30s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2940/3000 | Train Loss: 1.2495 | Val Loss: 1.2514 | Time: 1.28s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2945/3000 | Train Loss: 1.2465 | Val Loss: 1.2591 | Time: 2.13s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2950/3000 | Train Loss: 1.2688 | Val Loss: 1.2477 | Time: 1.45s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2955/3000 | Train Loss: 1.2567 | Val Loss: 1.2439 | Time: 1.38s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2960/3000 | Train Loss: 1.2681 | Val Loss: 1.2639 | Time: 1.70s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2965/3000 | Train Loss: 1.2520 | Val Loss: 1.2658 | Time: 1.33s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2970/3000 | Train Loss: 1.2628 | Val Loss: 1.2409 | Time: 1.41s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2975/3000 | Train Loss: 1.2635 | Val Loss: 1.2487 | Time: 1.96s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2980/3000 | Train Loss: 1.2604 | Val Loss: 1.2600 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2985/3000 | Train Loss: 1.2534 | Val Loss: 1.2883 | Time: 1.26s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2990/3000 | Train Loss: 1.2560 | Val Loss: 1.2816 | Time: 1.19s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 2995/3000 | Train Loss: 1.2439 | Val Loss: 1.2382 | Time: 1.18s\n",
      "[GINE_V2_Dropout_LeakyReLU] Epoch 3000/3000 | Train Loss: 1.2598 | Val Loss: 1.2620 | Time: 1.21s\n",
      "[GINE_V2_Dropout_LeakyReLU] FINAL TEST | MSE: 1.0826 | MAE: 0.6490 | R^2: 0.7339\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m      2\u001b[0m gine_v2_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgine\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_channels\u001b[39m\u001b[38;5;124m'\u001b[39m: in_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_norm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     14\u001b[0m gine_v2_model, gine_v2_info \u001b[38;5;241m=\u001b[39m run_experiment(\n\u001b[1;32m     15\u001b[0m     variation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGINE_V2_Dropout_LeakyReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     model_params\u001b[38;5;241m=\u001b[39mgine_v2_params,\n\u001b[1;32m     17\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m,\n\u001b[1;32m     18\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(gine_v2_model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43msave_dir\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgine_v2_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(gine_v2_info,              os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgine_v2_info.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_dir' is not defined"
     ]
    }
   ],
   "source": [
    "### Variation 2: Dropout + LeakyReLU\n",
    "gine_v2_params = {\n",
    "    'model_name': 'gine',\n",
    "    'in_channels': in_channels,\n",
    "    'hidden_dim': 64,\n",
    "    'out_channels': 1,\n",
    "    'dropout': 0.3,\n",
    "    'activation': 'leakyrelu',\n",
    "    'pool': 'mean',\n",
    "    'residual': False,\n",
    "    'batch_norm': False\n",
    "}\n",
    "\n",
    "gine_v2_model, gine_v2_info = run_experiment(\n",
    "    variation_name=\"GINE_V2_Dropout_LeakyReLU\",\n",
    "    model_params=gine_v2_params,\n",
    "    epochs=3000,\n",
    "    lr=0.001\n",
    ")\n",
    "torch.save(gine_v2_model.state_dict(), os.path.join(save_dir, \"gine_v2_model.pt\"))\n",
    "torch.save(gine_v2_info,              os.path.join(save_dir, \"gine_v2_info.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "save_dir = \"../data/experiments_\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "torch.save(gine_v2_model.state_dict(), os.path.join(save_dir, \"gine_v2_model.pt\"))\n",
    "torch.save(gine_v2_info,              os.path.join(save_dir, \"gine_v2_info.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
