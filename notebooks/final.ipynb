{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/data_utils.py\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_zinc_dataset(root='../data/ZINC', batch_size=64, subset=True):\n",
    "    \"\"\"\n",
    "    Loads the ZINC dataset from the specified root directory.\n",
    "\n",
    "    Args:\n",
    "        root (str): Path to the dataset folder.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        (DataLoader, DataLoader, DataLoader): train, val, and test loaders.\n",
    "    \"\"\"\n",
    "    train_dataset = ZINC(root, split='train', subset=subset)\n",
    "    val_dataset = ZINC(root, split='val', subset=subset)\n",
    "    test_dataset = ZINC(root, split='test', subset=subset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "def get_activation_fn(name):\n",
    "    name = name.lower()\n",
    "    if name == 'relu':\n",
    "        return F.relu\n",
    "    elif name == 'leakyrelu':\n",
    "        return F.leaky_relu\n",
    "    elif name == 'elu':\n",
    "        return F.elu\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {name}\")\n",
    "\n",
    "\n",
    "def get_activation_module(name):\n",
    "    \"\"\"\n",
    "    For usage in nn.Sequential, we need an nn.Module (e.g. nn.ReLU).\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    if name == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif name == 'leakyrelu':\n",
    "        return nn.LeakyReLU(negative_slope=0.2)\n",
    "    elif name == 'elu':\n",
    "        return nn.ELU()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation module: {name}\")\n",
    "\n",
    "\n",
    "\n",
    "# src/train.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_data in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass edge_attr to the model\n",
    "        out = model(\n",
    "            x=batch_data.x.float(),\n",
    "            edge_index=batch_data.edge_index,\n",
    "            batch=batch_data.batch,\n",
    "            edge_attr=batch_data.edge_attr.float()  # <-- Added\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        loss = criterion(out, batch_data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_data in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        out = model(\n",
    "            x=batch_data.x.float(),\n",
    "            edge_index=batch_data.edge_index,\n",
    "            batch=batch_data.batch,\n",
    "            edge_attr=batch_data.edge_attr.float()  # <-- Added\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        loss = criterion(out, batch_data.y.float())\n",
    "        total_loss += loss.item() * batch_data.num_graphs\n",
    "\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Return predictions and targets for analysis (plotting, etc).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    for batch_data in dataloader:\n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        out = model(\n",
    "            x=batch_data.x.float(),\n",
    "            edge_index=batch_data.edge_index,\n",
    "            batch=batch_data.batch,\n",
    "            edge_attr=batch_data.edge_attr.float()  # <-- Added\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        all_preds.append(out.detach().cpu())\n",
    "        all_targets.append(batch_data.y.cpu())\n",
    "\n",
    "    return torch.cat(all_preds), torch.cat(all_targets)\n",
    "\n",
    "\n",
    "# src/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv,\n",
    "    GINConv,\n",
    "    GATConv,\n",
    "    SAGEConv,\n",
    "    GINEConv,\n",
    "    global_mean_pool,\n",
    "    global_max_pool,\n",
    "    GlobalAttention\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# Pooling Helpers\n",
    "###############################################################################\n",
    "def get_pooling_fn(pool_type, hidden_dim):\n",
    "    pool_type = pool_type.lower()\n",
    "    if pool_type == 'mean':\n",
    "        return global_mean_pool\n",
    "    elif pool_type == 'max':\n",
    "        return global_max_pool\n",
    "    elif pool_type == 'attention':\n",
    "        gate_nn = nn.Sequential(nn.Linear(hidden_dim, 1))\n",
    "        return GlobalAttention(gate_nn)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling type: {pool_type}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1. GCN Model (no edge features)\n",
    "###############################################################################\n",
    "class GCNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible 2-layer GCN supporting dropout, batch norm, residual, etc.\n",
    "    This model ignores edge_attr entirely.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(GCNModel, self).__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # We ignore edge_attr for standard GCN\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. GIN Model (no edge features)\n",
    "###############################################################################\n",
    "class GINModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard 2-layer GIN ignoring edge_attr.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(GINModel, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv1 = GINConv(self.mlp1)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv2 = GINConv(self.mlp2)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # We ignore edge_attr for standard GIN\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3. GINE Model (USES edge features)\n",
    "###############################################################################\n",
    "class GINEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GINE variant that can incorporate edge_attr (e.g., bond types).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False,\n",
    "        edge_dim=None  # dimension of edge_attr, if known\n",
    "    ):\n",
    "        super(GINEModel, self).__init__()\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # MLP for first GINEConv\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv1 = GINEConv(nn=self.mlp1, edge_dim=edge_dim if edge_dim else 0)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # MLP for second GINEConv\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            get_activation_module(activation),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.conv2 = GINEConv(nn=self.mlp2, edge_dim=edge_dim if edge_dim else 0)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # Fix dimension if edge_attr is 1D\n",
    "        if edge_attr is not None and edge_attr.dim() == 1:\n",
    "            edge_attr = edge_attr.unsqueeze(-1)\n",
    "\n",
    "        # 1st GINEConv\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        # 2nd GINEConv\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. GAT Model (no edge features)\n",
    "###############################################################################\n",
    "class GATModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible 2-layer GAT supporting dropout, batch norm, residual, etc.\n",
    "    Ignores edge_attr unless you implement a custom attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        heads=4,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(GATModel, self).__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.conv1 = GATConv(in_channels, hidden_dim, heads=heads, concat=True)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=True)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim * heads)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. GraphSAGE Model (no edge features)\n",
    "###############################################################################\n",
    "class SAGEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible 2-layer GraphSAGE supporting dropout, batch norm, residual, etc.\n",
    "    Ignores edge_attr unless you implement a custom aggregator.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_dim=64,\n",
    "        out_channels=1,\n",
    "        dropout=0.0,\n",
    "        activation='relu',\n",
    "        pool='mean',\n",
    "        residual=False,\n",
    "        batch_norm=False\n",
    "    ):\n",
    "        super(SAGEModel, self).__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.activation_fn = get_activation_fn(activation)\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.pool = get_pooling_fn(pool, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        x_in = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x_in = x\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.activation_fn(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        if self.residual and x.shape == x_in.shape:\n",
    "            x = x + x_in\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6. Graph Transformer (no edge features by default)\n",
    "###############################################################################\n",
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Placeholder for a Graph Transformer approach.\n",
    "    Currently does not incorporate edge_attr in attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_dim=64, out_channels=1, num_heads=4):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.lin_in = nn.Linear(in_channels, hidden_dim)\n",
    "        self.lin_out = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        # We ignore edge_index and edge_attr in this placeholder\n",
    "        x = self.lin_in(x)\n",
    "        x = x.unsqueeze(1)           # [num_nodes, 1, hidden_dim]\n",
    "        x = x.permute(0, 1, 2)       # [num_nodes, 1, hidden_dim]\n",
    "        x = self.encoder(x)          # [num_nodes, 1, hidden_dim]\n",
    "        x = x.squeeze(1)             # [num_nodes, hidden_dim]\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7. Factory Method\n",
    "###############################################################################\n",
    "def get_model(\n",
    "    model_name,\n",
    "    in_channels,\n",
    "    hidden_dim=64,\n",
    "    out_channels=1,\n",
    "    dropout=0.0,\n",
    "    activation='relu',\n",
    "    pool='mean',\n",
    "    residual=False,\n",
    "    batch_norm=False,\n",
    "    heads=4,    # used for GAT/Transformer\n",
    "    edge_dim=None  # used for GINE\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns an instance of the requested model by name.\n",
    "    - 'gcn', 'gin', 'gat', 'sage', 'transformer' ignore edge_attr\n",
    "    - 'gine' uses edge_attr\n",
    "    \"\"\"\n",
    "    model_name = model_name.lower()\n",
    "\n",
    "    if model_name == 'gcn':\n",
    "        return GCNModel(\n",
    "            in_channels,\n",
    "            hidden_dim,\n",
    "            out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'gin':\n",
    "        return GINModel(\n",
    "            in_channels,\n",
    "            hidden_dim,\n",
    "            out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'gine':\n",
    "        return GINEModel(\n",
    "            in_channels,\n",
    "            hidden_dim,\n",
    "            out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm,\n",
    "            edge_dim=edge_dim\n",
    "        )\n",
    "    elif model_name == 'gat':\n",
    "        return GATModel(\n",
    "            in_channels=in_channels,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_channels=out_channels,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'sage':\n",
    "        return SAGEModel(\n",
    "            in_channels=in_channels,\n",
    "            hidden_dim=hidden_dim,\n",
    "            out_channels=out_channels,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            pool=pool,\n",
    "            residual=residual,\n",
    "            batch_norm=batch_norm\n",
    "        )\n",
    "    elif model_name == 'transformer':\n",
    "        return GraphTransformer(\n",
    "            in_channels,\n",
    "            hidden_dim,\n",
    "            out_channels,\n",
    "            num_heads=heads\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Inferred in_channels: 1\n"
     ]
    }
   ],
   "source": [
    "# notebooks/Run_All_Models.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# If needed:\n",
    "# sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 1. LOAD DATA ONCE FOR ALL MODELS\n",
    "###############################################################################\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = get_zinc_dataset(\n",
    "    root='../data/ZINC',\n",
    "    batch_size=batch_size,\n",
    "    subset=False\n",
    ")\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "in_channels = sample_batch.x.size(-1)\n",
    "print(f\"Inferred in_channels: {in_channels}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2. UTILITY FUNCTION TO RUN EXPERIMENT\n",
    "###############################################################################\n",
    "def run_experiment(variation_name, model_params, epochs=25, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model given by model_params, for 'epochs' epochs,\n",
    "    using MSE loss + Adam at LR=lr. Returns (model, run_info).\n",
    "    \"\"\"\n",
    "    model = get_model(**model_params).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # We'll store results here for logging/analysis\n",
    "    run_info = {\n",
    "        'variation': variation_name,\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'epoch_time': [],\n",
    "        # 'num_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        run_info['epoch'].append(epoch)\n",
    "        run_info['train_loss'].append(train_loss)\n",
    "        run_info['val_loss'].append(val_loss)\n",
    "        run_info['epoch_time'].append(epoch_time)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"[{variation_name}] Epoch {epoch}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Final Test\n",
    "    test_loss = evaluate(model, test_loader, criterion, device)\n",
    "    run_info['test_loss'] = test_loss\n",
    "\n",
    "    # Additional metrics\n",
    "    preds, targets = predict(model, test_loader, device)\n",
    "    preds_np, targets_np = preds.numpy(), targets.numpy()\n",
    "    run_info['test_mae'] = mean_absolute_error(targets_np, preds_np)\n",
    "    run_info['test_r2']  = r2_score(targets_np, preds_np)\n",
    "\n",
    "    print(f\"[{variation_name}] FINAL TEST | \"\n",
    "          f\"MSE: {test_loss:.4f} | MAE: {run_info['test_mae']:.4f} | R^2: {run_info['test_r2']:.4f}\")\n",
    "\n",
    "    return model, run_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "save_dir = \"../data/experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/GNN/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GINE_V3_Attn_Residual_BN] Epoch 5/1500 | Train Loss: 1.7294 | Val Loss: 1.5722 | Time: 29.65s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 10/1500 | Train Loss: 1.4866 | Val Loss: 1.6993 | Time: 28.01s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 15/1500 | Train Loss: 1.4178 | Val Loss: 1.2947 | Time: 36.87s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 20/1500 | Train Loss: 1.3847 | Val Loss: 1.3407 | Time: 46.09s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 25/1500 | Train Loss: 1.3602 | Val Loss: 1.2531 | Time: 35.78s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 30/1500 | Train Loss: 1.3394 | Val Loss: 1.2226 | Time: 42.74s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 35/1500 | Train Loss: 1.3217 | Val Loss: 1.2225 | Time: 35.81s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 40/1500 | Train Loss: 1.3148 | Val Loss: 1.2661 | Time: 33.30s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 45/1500 | Train Loss: 1.3084 | Val Loss: 1.3666 | Time: 42.84s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 50/1500 | Train Loss: 1.2953 | Val Loss: 1.1792 | Time: 37.95s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 55/1500 | Train Loss: 1.2882 | Val Loss: 1.1904 | Time: 40.12s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 60/1500 | Train Loss: 1.2837 | Val Loss: 1.2852 | Time: 42.46s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 65/1500 | Train Loss: 1.2806 | Val Loss: 1.1760 | Time: 39.50s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 70/1500 | Train Loss: 1.2753 | Val Loss: 1.2148 | Time: 33.59s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 75/1500 | Train Loss: 1.2686 | Val Loss: 1.2460 | Time: 30.05s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 80/1500 | Train Loss: 1.2665 | Val Loss: 1.2304 | Time: 28.87s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 85/1500 | Train Loss: 1.2673 | Val Loss: 1.1830 | Time: 29.36s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 90/1500 | Train Loss: 1.2638 | Val Loss: 1.1554 | Time: 29.32s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 95/1500 | Train Loss: 1.2599 | Val Loss: 1.1777 | Time: 29.38s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 100/1500 | Train Loss: 1.2558 | Val Loss: 1.1845 | Time: 29.31s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 105/1500 | Train Loss: 1.2521 | Val Loss: 1.1651 | Time: 29.78s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 110/1500 | Train Loss: 1.2501 | Val Loss: 1.1649 | Time: 29.43s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 115/1500 | Train Loss: 1.2440 | Val Loss: 1.1515 | Time: 29.01s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 120/1500 | Train Loss: 1.2445 | Val Loss: 1.1512 | Time: 29.46s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 125/1500 | Train Loss: 1.2404 | Val Loss: 1.2247 | Time: 30.38s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 130/1500 | Train Loss: 1.2391 | Val Loss: 1.1373 | Time: 29.98s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 135/1500 | Train Loss: 1.2379 | Val Loss: 1.1977 | Time: 29.74s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 140/1500 | Train Loss: 1.2369 | Val Loss: 1.1681 | Time: 29.66s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 145/1500 | Train Loss: 1.2345 | Val Loss: 1.2179 | Time: 32.89s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 150/1500 | Train Loss: 1.2318 | Val Loss: 1.1366 | Time: 32.39s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 155/1500 | Train Loss: 1.2298 | Val Loss: 1.2301 | Time: 31.93s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 160/1500 | Train Loss: 1.2293 | Val Loss: 1.1894 | Time: 31.56s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 165/1500 | Train Loss: 1.2266 | Val Loss: 1.1294 | Time: 31.45s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 170/1500 | Train Loss: 1.2218 | Val Loss: 1.1718 | Time: 31.26s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 175/1500 | Train Loss: 1.2216 | Val Loss: 1.1713 | Time: 40.75s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 180/1500 | Train Loss: 1.2228 | Val Loss: 1.2213 | Time: 48.78s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 185/1500 | Train Loss: 1.2129 | Val Loss: 1.2154 | Time: 34.26s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 190/1500 | Train Loss: 1.2136 | Val Loss: 1.1829 | Time: 31.24s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 195/1500 | Train Loss: 1.2147 | Val Loss: 1.1269 | Time: 33.91s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 200/1500 | Train Loss: 1.2084 | Val Loss: 1.1160 | Time: 36.11s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 205/1500 | Train Loss: 1.2115 | Val Loss: 1.1173 | Time: 33.76s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 210/1500 | Train Loss: 1.2090 | Val Loss: 1.1049 | Time: 36.95s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 215/1500 | Train Loss: 1.2053 | Val Loss: 1.1086 | Time: 34.95s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 220/1500 | Train Loss: 1.2059 | Val Loss: 1.1080 | Time: 35.29s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 225/1500 | Train Loss: 1.2047 | Val Loss: 1.1248 | Time: 43.83s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 230/1500 | Train Loss: 1.2009 | Val Loss: 1.3534 | Time: 33.20s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 235/1500 | Train Loss: 1.2001 | Val Loss: 1.1663 | Time: 35.54s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 240/1500 | Train Loss: 1.1994 | Val Loss: 1.1785 | Time: 35.51s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 245/1500 | Train Loss: 1.1978 | Val Loss: 1.1613 | Time: 36.98s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 250/1500 | Train Loss: 1.1973 | Val Loss: 1.1183 | Time: 33.94s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 255/1500 | Train Loss: 1.1928 | Val Loss: 1.0961 | Time: 32.35s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 260/1500 | Train Loss: 1.1908 | Val Loss: 1.1139 | Time: 31.48s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 265/1500 | Train Loss: 1.1930 | Val Loss: 1.1089 | Time: 37.30s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 270/1500 | Train Loss: 1.1906 | Val Loss: 1.1124 | Time: 32.33s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 275/1500 | Train Loss: 1.1884 | Val Loss: 1.1110 | Time: 32.66s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 280/1500 | Train Loss: 1.1896 | Val Loss: 1.1158 | Time: 31.78s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 285/1500 | Train Loss: 1.1877 | Val Loss: 1.1160 | Time: 32.62s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 290/1500 | Train Loss: 1.1865 | Val Loss: 1.1890 | Time: 35.22s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 295/1500 | Train Loss: 1.1873 | Val Loss: 1.1127 | Time: 31.09s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 300/1500 | Train Loss: 1.1845 | Val Loss: 1.1446 | Time: 33.18s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 305/1500 | Train Loss: 1.1818 | Val Loss: 1.0948 | Time: 30.60s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 310/1500 | Train Loss: 1.1831 | Val Loss: 1.1146 | Time: 31.32s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 315/1500 | Train Loss: 1.1792 | Val Loss: 1.1980 | Time: 34.90s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 320/1500 | Train Loss: 1.1808 | Val Loss: 1.0727 | Time: 62.22s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 325/1500 | Train Loss: 1.1779 | Val Loss: 1.0838 | Time: 39.01s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 330/1500 | Train Loss: 1.1761 | Val Loss: 1.0978 | Time: 37.92s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 335/1500 | Train Loss: 1.1758 | Val Loss: 1.1259 | Time: 58.29s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 340/1500 | Train Loss: 1.1817 | Val Loss: 1.1222 | Time: 33.05s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 345/1500 | Train Loss: 1.1793 | Val Loss: 1.1266 | Time: 33.22s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 350/1500 | Train Loss: 1.1776 | Val Loss: 1.1110 | Time: 35.01s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 355/1500 | Train Loss: 1.1725 | Val Loss: 1.0894 | Time: 30.56s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 360/1500 | Train Loss: 1.1738 | Val Loss: 1.0882 | Time: 54.41s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 365/1500 | Train Loss: 1.1692 | Val Loss: 1.1160 | Time: 57.05s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 370/1500 | Train Loss: 1.1699 | Val Loss: 1.1264 | Time: 34.50s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 375/1500 | Train Loss: 1.1721 | Val Loss: 1.1312 | Time: 34.07s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 380/1500 | Train Loss: 1.1721 | Val Loss: 1.0961 | Time: 32.40s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 385/1500 | Train Loss: 1.1685 | Val Loss: 1.0973 | Time: 35.61s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 390/1500 | Train Loss: 1.1668 | Val Loss: 1.0998 | Time: 35.82s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 395/1500 | Train Loss: 1.1687 | Val Loss: 1.1970 | Time: 32.99s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 400/1500 | Train Loss: 1.1738 | Val Loss: 1.0923 | Time: 40.82s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 405/1500 | Train Loss: 1.1651 | Val Loss: 1.0876 | Time: 35.49s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 410/1500 | Train Loss: 1.1649 | Val Loss: 1.0751 | Time: 42.65s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 415/1500 | Train Loss: 1.1661 | Val Loss: 1.0966 | Time: 35.51s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 420/1500 | Train Loss: 1.1653 | Val Loss: 1.0760 | Time: 35.23s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 425/1500 | Train Loss: 1.1645 | Val Loss: 1.0803 | Time: 30.63s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 430/1500 | Train Loss: 1.1672 | Val Loss: 1.0852 | Time: 30.14s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 435/1500 | Train Loss: 1.1608 | Val Loss: 1.0847 | Time: 30.16s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 440/1500 | Train Loss: 1.1585 | Val Loss: 1.0676 | Time: 30.13s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 445/1500 | Train Loss: 1.1596 | Val Loss: 1.1065 | Time: 31.19s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 450/1500 | Train Loss: 1.1611 | Val Loss: 1.1143 | Time: 30.06s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 455/1500 | Train Loss: 1.1594 | Val Loss: 1.1067 | Time: 30.48s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 460/1500 | Train Loss: 1.1630 | Val Loss: 1.0807 | Time: 30.95s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 465/1500 | Train Loss: 1.1618 | Val Loss: 1.1579 | Time: 31.69s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 470/1500 | Train Loss: 1.1605 | Val Loss: 1.0952 | Time: 30.59s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 475/1500 | Train Loss: 1.1598 | Val Loss: 1.0818 | Time: 31.12s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 480/1500 | Train Loss: 1.1582 | Val Loss: 1.1669 | Time: 30.56s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 485/1500 | Train Loss: 1.1569 | Val Loss: 1.2337 | Time: 30.52s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 490/1500 | Train Loss: 1.1588 | Val Loss: 1.1138 | Time: 29.14s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 495/1500 | Train Loss: 1.1583 | Val Loss: 1.1556 | Time: 31.54s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 500/1500 | Train Loss: 1.1532 | Val Loss: 1.1168 | Time: 30.07s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 505/1500 | Train Loss: 1.1553 | Val Loss: 1.2158 | Time: 31.41s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 510/1500 | Train Loss: 1.1535 | Val Loss: 1.1703 | Time: 29.17s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 515/1500 | Train Loss: 1.1519 | Val Loss: 1.0832 | Time: 31.59s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 520/1500 | Train Loss: 1.1541 | Val Loss: 1.0580 | Time: 28.92s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 525/1500 | Train Loss: 1.1529 | Val Loss: 1.0631 | Time: 30.32s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 530/1500 | Train Loss: 1.1560 | Val Loss: 1.0671 | Time: 29.41s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 535/1500 | Train Loss: 1.1519 | Val Loss: 1.0561 | Time: 31.48s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 540/1500 | Train Loss: 1.1498 | Val Loss: 1.1355 | Time: 30.56s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 545/1500 | Train Loss: 1.1498 | Val Loss: 1.0821 | Time: 30.77s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 550/1500 | Train Loss: 1.1514 | Val Loss: 1.0674 | Time: 30.16s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 555/1500 | Train Loss: 1.1480 | Val Loss: 1.0865 | Time: 31.02s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 560/1500 | Train Loss: 1.1484 | Val Loss: 1.0571 | Time: 30.29s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 565/1500 | Train Loss: 1.1479 | Val Loss: 1.0916 | Time: 31.37s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 570/1500 | Train Loss: 1.1457 | Val Loss: 1.1127 | Time: 29.29s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 575/1500 | Train Loss: 1.1421 | Val Loss: 1.0664 | Time: 30.38s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 580/1500 | Train Loss: 1.1442 | Val Loss: 1.0665 | Time: 29.53s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 585/1500 | Train Loss: 1.1411 | Val Loss: 1.0898 | Time: 30.52s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 590/1500 | Train Loss: 1.1460 | Val Loss: 1.0946 | Time: 30.40s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 595/1500 | Train Loss: 1.1399 | Val Loss: 1.1164 | Time: 30.19s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 600/1500 | Train Loss: 1.1424 | Val Loss: 1.0544 | Time: 29.40s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 605/1500 | Train Loss: 1.1416 | Val Loss: 1.0879 | Time: 30.94s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 610/1500 | Train Loss: 1.1435 | Val Loss: 1.0761 | Time: 30.80s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 615/1500 | Train Loss: 1.1396 | Val Loss: 1.0631 | Time: 30.54s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 620/1500 | Train Loss: 1.1455 | Val Loss: 1.0726 | Time: 29.42s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 625/1500 | Train Loss: 1.1443 | Val Loss: 1.0653 | Time: 33.56s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 630/1500 | Train Loss: 1.1401 | Val Loss: 1.0623 | Time: 29.34s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 635/1500 | Train Loss: 1.1397 | Val Loss: 1.0558 | Time: 29.35s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 640/1500 | Train Loss: 1.1398 | Val Loss: 1.0787 | Time: 29.27s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 645/1500 | Train Loss: 1.1381 | Val Loss: 1.0887 | Time: 30.90s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 650/1500 | Train Loss: 1.1393 | Val Loss: 1.0850 | Time: 30.42s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 655/1500 | Train Loss: 1.1433 | Val Loss: 1.0708 | Time: 30.03s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 660/1500 | Train Loss: 1.1386 | Val Loss: 1.0656 | Time: 30.58s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 665/1500 | Train Loss: 1.1405 | Val Loss: 1.0585 | Time: 29.66s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 670/1500 | Train Loss: 1.1392 | Val Loss: 1.0607 | Time: 28.95s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 675/1500 | Train Loss: 1.1392 | Val Loss: 1.0716 | Time: 30.75s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 680/1500 | Train Loss: 1.1360 | Val Loss: 1.0568 | Time: 30.14s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 685/1500 | Train Loss: 1.1338 | Val Loss: 1.0576 | Time: 32.58s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 690/1500 | Train Loss: 1.1389 | Val Loss: 1.0595 | Time: 29.82s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 695/1500 | Train Loss: 1.1381 | Val Loss: 1.0605 | Time: 32.33s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 700/1500 | Train Loss: 1.1353 | Val Loss: 1.0836 | Time: 29.43s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 705/1500 | Train Loss: 1.1344 | Val Loss: 1.0603 | Time: 29.48s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 710/1500 | Train Loss: 1.1325 | Val Loss: 1.1156 | Time: 30.36s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 715/1500 | Train Loss: 1.1337 | Val Loss: 1.0568 | Time: 30.26s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 720/1500 | Train Loss: 1.1339 | Val Loss: 1.0657 | Time: 29.67s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 725/1500 | Train Loss: 1.1342 | Val Loss: 1.0628 | Time: 30.08s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 730/1500 | Train Loss: 1.1369 | Val Loss: 1.0497 | Time: 29.69s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 735/1500 | Train Loss: 1.1351 | Val Loss: 1.0536 | Time: 29.64s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 740/1500 | Train Loss: 1.1367 | Val Loss: 1.0498 | Time: 30.01s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 745/1500 | Train Loss: 1.1329 | Val Loss: 1.0532 | Time: 30.57s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 750/1500 | Train Loss: 1.1297 | Val Loss: 1.1234 | Time: 29.42s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 755/1500 | Train Loss: 1.1355 | Val Loss: 1.0454 | Time: 29.47s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 760/1500 | Train Loss: 1.1311 | Val Loss: 1.1039 | Time: 29.78s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 765/1500 | Train Loss: 1.1305 | Val Loss: 1.0754 | Time: 29.03s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 770/1500 | Train Loss: 1.1269 | Val Loss: 1.0504 | Time: 29.45s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 775/1500 | Train Loss: 1.1241 | Val Loss: 1.2064 | Time: 29.59s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 780/1500 | Train Loss: 1.1282 | Val Loss: 1.0607 | Time: 29.68s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 785/1500 | Train Loss: 1.1276 | Val Loss: 1.0844 | Time: 28.86s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 790/1500 | Train Loss: 1.1294 | Val Loss: 1.0441 | Time: 28.84s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 795/1500 | Train Loss: 1.1319 | Val Loss: 1.0472 | Time: 29.05s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 800/1500 | Train Loss: 1.1262 | Val Loss: 1.0464 | Time: 29.65s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 805/1500 | Train Loss: 1.1279 | Val Loss: 1.0639 | Time: 32.04s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 810/1500 | Train Loss: 1.1300 | Val Loss: 1.0751 | Time: 29.69s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 815/1500 | Train Loss: 1.1263 | Val Loss: 1.0500 | Time: 29.40s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 820/1500 | Train Loss: 1.1296 | Val Loss: 1.1166 | Time: 30.06s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 825/1500 | Train Loss: 1.1351 | Val Loss: 1.1591 | Time: 29.31s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 830/1500 | Train Loss: 1.1299 | Val Loss: 1.0586 | Time: 29.20s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 835/1500 | Train Loss: 1.1268 | Val Loss: 1.0507 | Time: 29.17s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 840/1500 | Train Loss: 1.1232 | Val Loss: 1.0659 | Time: 29.67s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 845/1500 | Train Loss: 1.1218 | Val Loss: 1.0703 | Time: 29.17s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 850/1500 | Train Loss: 1.1238 | Val Loss: 1.0682 | Time: 29.29s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 855/1500 | Train Loss: 1.1234 | Val Loss: 1.0540 | Time: 29.12s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 860/1500 | Train Loss: 1.1278 | Val Loss: 1.0579 | Time: 30.13s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 865/1500 | Train Loss: 1.1245 | Val Loss: 1.0720 | Time: 31.42s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 870/1500 | Train Loss: 1.1256 | Val Loss: 1.0645 | Time: 29.14s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 875/1500 | Train Loss: 1.1248 | Val Loss: 1.0665 | Time: 29.72s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 880/1500 | Train Loss: 1.1214 | Val Loss: 1.0841 | Time: 30.48s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 885/1500 | Train Loss: 1.1212 | Val Loss: 1.1575 | Time: 30.00s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 890/1500 | Train Loss: 1.1246 | Val Loss: 1.0677 | Time: 29.52s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 895/1500 | Train Loss: 1.1235 | Val Loss: 1.0498 | Time: 29.67s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 900/1500 | Train Loss: 1.1249 | Val Loss: 1.0399 | Time: 29.34s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 905/1500 | Train Loss: 1.1249 | Val Loss: 1.0490 | Time: 29.25s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 910/1500 | Train Loss: 1.1230 | Val Loss: 1.0572 | Time: 29.37s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 915/1500 | Train Loss: 1.1234 | Val Loss: 1.0650 | Time: 29.15s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 920/1500 | Train Loss: 1.1228 | Val Loss: 1.0485 | Time: 29.62s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 925/1500 | Train Loss: 1.1152 | Val Loss: 1.0461 | Time: 29.87s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 930/1500 | Train Loss: 1.1229 | Val Loss: 1.0568 | Time: 28.86s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 935/1500 | Train Loss: 1.1228 | Val Loss: 1.0309 | Time: 29.26s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 940/1500 | Train Loss: 1.1209 | Val Loss: 1.0454 | Time: 29.66s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 945/1500 | Train Loss: 1.1229 | Val Loss: 1.0777 | Time: 29.77s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 950/1500 | Train Loss: 1.1198 | Val Loss: 1.0587 | Time: 29.80s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 955/1500 | Train Loss: 1.1172 | Val Loss: 1.0776 | Time: 29.95s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 960/1500 | Train Loss: 1.1242 | Val Loss: 1.0438 | Time: 29.36s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 965/1500 | Train Loss: 1.1240 | Val Loss: 1.0433 | Time: 29.39s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 970/1500 | Train Loss: 1.1189 | Val Loss: 1.0531 | Time: 29.65s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 975/1500 | Train Loss: 1.1229 | Val Loss: 1.0423 | Time: 29.46s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 980/1500 | Train Loss: 1.1198 | Val Loss: 1.0742 | Time: 29.41s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 985/1500 | Train Loss: 1.1226 | Val Loss: 1.0855 | Time: 29.81s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 990/1500 | Train Loss: 1.1225 | Val Loss: 1.0543 | Time: 28.74s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 995/1500 | Train Loss: 1.1188 | Val Loss: 1.0429 | Time: 29.10s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1000/1500 | Train Loss: 1.1248 | Val Loss: 1.0382 | Time: 29.99s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1005/1500 | Train Loss: 1.1190 | Val Loss: 1.0439 | Time: 29.38s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1010/1500 | Train Loss: 1.1202 | Val Loss: 1.0951 | Time: 29.05s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1015/1500 | Train Loss: 1.1179 | Val Loss: 1.0524 | Time: 29.79s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1020/1500 | Train Loss: 1.1213 | Val Loss: 1.0870 | Time: 29.69s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1025/1500 | Train Loss: 1.1157 | Val Loss: 1.0501 | Time: 29.24s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1030/1500 | Train Loss: 1.1200 | Val Loss: 1.0575 | Time: 29.59s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1035/1500 | Train Loss: 1.1201 | Val Loss: 1.0891 | Time: 29.81s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1040/1500 | Train Loss: 1.1215 | Val Loss: 1.0622 | Time: 29.46s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1045/1500 | Train Loss: 1.1179 | Val Loss: 1.0660 | Time: 30.83s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1050/1500 | Train Loss: 1.1194 | Val Loss: 1.0344 | Time: 28.58s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1055/1500 | Train Loss: 1.1222 | Val Loss: 1.0570 | Time: 29.37s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1060/1500 | Train Loss: 1.1183 | Val Loss: 1.0621 | Time: 30.24s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1065/1500 | Train Loss: 1.1192 | Val Loss: 1.0461 | Time: 29.09s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1070/1500 | Train Loss: 1.1145 | Val Loss: 1.0414 | Time: 28.94s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1075/1500 | Train Loss: 1.1144 | Val Loss: 1.0747 | Time: 29.06s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1080/1500 | Train Loss: 1.1146 | Val Loss: 1.0564 | Time: 28.80s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1085/1500 | Train Loss: 1.1177 | Val Loss: 1.0707 | Time: 29.96s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1090/1500 | Train Loss: 1.1167 | Val Loss: 1.0915 | Time: 29.14s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1095/1500 | Train Loss: 1.1173 | Val Loss: 1.0360 | Time: 28.92s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1100/1500 | Train Loss: 1.1180 | Val Loss: 1.0420 | Time: 30.19s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1105/1500 | Train Loss: 1.1176 | Val Loss: 1.0469 | Time: 29.37s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1110/1500 | Train Loss: 1.1156 | Val Loss: 1.0565 | Time: 29.39s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1115/1500 | Train Loss: 1.1166 | Val Loss: 1.0441 | Time: 29.85s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1120/1500 | Train Loss: 1.1148 | Val Loss: 1.0407 | Time: 29.25s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1125/1500 | Train Loss: 1.1165 | Val Loss: 1.0658 | Time: 29.46s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1130/1500 | Train Loss: 1.1166 | Val Loss: 1.0699 | Time: 29.02s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1135/1500 | Train Loss: 1.1224 | Val Loss: 1.0539 | Time: 28.77s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1140/1500 | Train Loss: 1.1183 | Val Loss: 1.0419 | Time: 29.10s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1145/1500 | Train Loss: 1.1106 | Val Loss: 1.0453 | Time: 29.31s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1150/1500 | Train Loss: 1.1135 | Val Loss: 1.0490 | Time: 29.16s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1155/1500 | Train Loss: 1.1162 | Val Loss: 1.0489 | Time: 29.31s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1160/1500 | Train Loss: 1.1118 | Val Loss: 1.0327 | Time: 29.28s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1165/1500 | Train Loss: 1.1150 | Val Loss: 1.0350 | Time: 31.31s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1170/1500 | Train Loss: 1.1138 | Val Loss: 1.0513 | Time: 29.38s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1175/1500 | Train Loss: 1.1165 | Val Loss: 1.0479 | Time: 29.10s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1180/1500 | Train Loss: 1.1099 | Val Loss: 1.0336 | Time: 28.79s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1185/1500 | Train Loss: 1.1162 | Val Loss: 1.0348 | Time: 29.72s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1190/1500 | Train Loss: 1.1203 | Val Loss: 1.0374 | Time: 29.21s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1195/1500 | Train Loss: 1.1131 | Val Loss: 1.0644 | Time: 29.29s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1200/1500 | Train Loss: 1.1167 | Val Loss: 1.0468 | Time: 28.76s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1205/1500 | Train Loss: 1.1099 | Val Loss: 1.1510 | Time: 29.54s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1210/1500 | Train Loss: 1.1139 | Val Loss: 1.0300 | Time: 29.02s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1215/1500 | Train Loss: 1.1097 | Val Loss: 1.0535 | Time: 29.54s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1220/1500 | Train Loss: 1.1131 | Val Loss: 1.0359 | Time: 29.45s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1225/1500 | Train Loss: 1.1115 | Val Loss: 1.0551 | Time: 33.85s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1230/1500 | Train Loss: 1.1094 | Val Loss: 1.0340 | Time: 28.98s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1235/1500 | Train Loss: 1.1127 | Val Loss: 1.0449 | Time: 30.09s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1240/1500 | Train Loss: 1.1122 | Val Loss: 1.0465 | Time: 29.13s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1245/1500 | Train Loss: 1.1136 | Val Loss: 1.0424 | Time: 29.49s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1250/1500 | Train Loss: 1.1127 | Val Loss: 1.0629 | Time: 29.20s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1255/1500 | Train Loss: 1.1154 | Val Loss: 1.0839 | Time: 29.35s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1260/1500 | Train Loss: 1.1138 | Val Loss: 1.0634 | Time: 29.12s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1265/1500 | Train Loss: 1.1136 | Val Loss: 1.0389 | Time: 29.27s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1270/1500 | Train Loss: 1.1090 | Val Loss: 1.0484 | Time: 29.08s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1275/1500 | Train Loss: 1.1099 | Val Loss: 1.0493 | Time: 30.53s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1280/1500 | Train Loss: 1.1159 | Val Loss: 1.0418 | Time: 29.08s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1285/1500 | Train Loss: 1.1061 | Val Loss: 1.0430 | Time: 31.85s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1290/1500 | Train Loss: 1.1064 | Val Loss: 1.0341 | Time: 28.53s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1295/1500 | Train Loss: 1.1090 | Val Loss: 1.0302 | Time: 28.82s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1300/1500 | Train Loss: 1.1108 | Val Loss: 1.0303 | Time: 28.88s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1305/1500 | Train Loss: 1.1096 | Val Loss: 1.0416 | Time: 29.12s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1310/1500 | Train Loss: 1.1120 | Val Loss: 1.0304 | Time: 28.84s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1315/1500 | Train Loss: 1.1093 | Val Loss: 1.0640 | Time: 28.97s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1320/1500 | Train Loss: 1.1078 | Val Loss: 1.0379 | Time: 28.54s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1325/1500 | Train Loss: 1.1079 | Val Loss: 1.0371 | Time: 29.44s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1330/1500 | Train Loss: 1.1087 | Val Loss: 1.0239 | Time: 28.13s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1335/1500 | Train Loss: 1.1046 | Val Loss: 1.0427 | Time: 28.87s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1340/1500 | Train Loss: 1.1094 | Val Loss: 1.0207 | Time: 28.35s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1345/1500 | Train Loss: 1.1085 | Val Loss: 1.0568 | Time: 29.28s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1350/1500 | Train Loss: 1.1074 | Val Loss: 1.0287 | Time: 30.97s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1355/1500 | Train Loss: 1.1101 | Val Loss: 1.0560 | Time: 29.25s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1360/1500 | Train Loss: 1.1107 | Val Loss: 1.0459 | Time: 29.59s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1365/1500 | Train Loss: 1.1028 | Val Loss: 1.0334 | Time: 29.30s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1370/1500 | Train Loss: 1.1108 | Val Loss: 1.0429 | Time: 28.52s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1375/1500 | Train Loss: 1.1104 | Val Loss: 1.0648 | Time: 29.89s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1380/1500 | Train Loss: 1.1091 | Val Loss: 1.0499 | Time: 29.75s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1385/1500 | Train Loss: 1.1107 | Val Loss: 1.0418 | Time: 29.11s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1390/1500 | Train Loss: 1.1070 | Val Loss: 1.1325 | Time: 28.55s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1395/1500 | Train Loss: 1.1101 | Val Loss: 1.0651 | Time: 28.98s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1400/1500 | Train Loss: 1.1058 | Val Loss: 1.0356 | Time: 29.42s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1405/1500 | Train Loss: 1.1092 | Val Loss: 1.0295 | Time: 28.96s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1410/1500 | Train Loss: 1.1092 | Val Loss: 1.0423 | Time: 30.07s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1415/1500 | Train Loss: 1.1030 | Val Loss: 1.1151 | Time: 29.59s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1420/1500 | Train Loss: 1.1088 | Val Loss: 1.0489 | Time: 28.97s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1425/1500 | Train Loss: 1.1045 | Val Loss: 1.0482 | Time: 29.35s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1430/1500 | Train Loss: 1.1107 | Val Loss: 1.0365 | Time: 29.72s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1435/1500 | Train Loss: 1.1048 | Val Loss: 1.0849 | Time: 28.98s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1440/1500 | Train Loss: 1.1087 | Val Loss: 1.0413 | Time: 28.79s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1445/1500 | Train Loss: 1.1064 | Val Loss: 1.0471 | Time: 29.11s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1450/1500 | Train Loss: 1.1066 | Val Loss: 1.0339 | Time: 29.24s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1455/1500 | Train Loss: 1.1085 | Val Loss: 1.0333 | Time: 28.57s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1460/1500 | Train Loss: 1.1084 | Val Loss: 1.0522 | Time: 28.98s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1465/1500 | Train Loss: 1.1072 | Val Loss: 1.0674 | Time: 28.81s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1470/1500 | Train Loss: 1.1026 | Val Loss: 1.0616 | Time: 29.52s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1475/1500 | Train Loss: 1.1065 | Val Loss: 1.0647 | Time: 28.68s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1480/1500 | Train Loss: 1.1050 | Val Loss: 1.0267 | Time: 29.12s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1485/1500 | Train Loss: 1.1030 | Val Loss: 1.0391 | Time: 28.25s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1490/1500 | Train Loss: 1.1060 | Val Loss: 1.0319 | Time: 30.07s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1495/1500 | Train Loss: 1.1080 | Val Loss: 1.0593 | Time: 28.73s\n",
      "[GINE_V3_Attn_Residual_BN] Epoch 1500/1500 | Train Loss: 1.1013 | Val Loss: 1.0872 | Time: 29.10s\n",
      "[GINE_V3_Attn_Residual_BN] FINAL TEST | MSE: 0.8035 | MAE: 0.5132 | R^2: 0.7967\n"
     ]
    }
   ],
   "source": [
    "gine_v3_params = {\n",
    "    'model_name': 'gine',\n",
    "    'in_channels': in_channels,\n",
    "    'hidden_dim': 64,\n",
    "    'out_channels': 1,\n",
    "    'dropout': 0.3,\n",
    "    'activation': 'leakyrelu',\n",
    "    'pool': 'attention',\n",
    "    'residual': True,\n",
    "    'batch_norm': True\n",
    "}\n",
    "\n",
    "gine_v3_model, gine_v3_info = run_experiment(\n",
    "    variation_name=\"GINE_V3_Attn_Residual_BN\",\n",
    "    model_params=gine_v3_params,\n",
    "    epochs=1500,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "torch.save(gine_v3_model.state_dict(), os.path.join(save_dir, \"final_model.pt\"))\n",
    "torch.save(gine_v3_info,  os.path.join(save_dir, \"final_info.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
